"""
Colab Setup Utility

This module provides automated setup functions for Google Colab environment,
including environment configuration, data verification, and preprocessing.
"""

import os
import sys
import subprocess
import gc
from pathlib import Path
from typing import Optional, Dict, Any
import logging
import numpy as np
import tempfile

# Set up logging robustly to ensure messages are always displayed
logger = logging.getLogger()
# If the logger has handlers, it's already been configured.
if not logger.handlers:
    logger.setLevel(logging.INFO)
    handler = logging.StreamHandler(sys.stdout)
    handler.setLevel(logging.INFO)
    formatter = logging.Formatter('%(name)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)

def setup_colab_environment(
    repo_url: str = "https://github.com/uncertainlyprincipaled/YaleGWI.git",
    install_extra_packages: bool = True,
    skip_package_install: bool = True  # NEW: Skip package installation
) -> Dict[str, Any]:
    """
    Set up the Colab environment with all necessary dependencies.
    
    Args:
        repo_url: URL of the repository to clone
        install_extra_packages: Whether to install additional packages for preprocessing
        skip_package_install: Whether to skip package installation (assumes setup_s3fs.py was run)
        
    Returns:
        Dict containing setup information
    """
    print("üöÄ Setting up Colab environment...")
    
    # Clone repository if not already present
    if not Path('/content/YaleGWI').exists():
        print("üì• Cloning repository...")
        subprocess.run(['git', 'clone', repo_url], check=True)
    else:
        print("üìÅ Repository already exists, skipping clone")
        # Try to update the repository if it's a git repo
        try:
            os.chdir('/content/YaleGWI')
            subprocess.run(['git', 'pull'], check=True)
            print("‚úÖ Repository updated")
        except subprocess.CalledProcessError:
            print("‚ö†Ô∏è Could not update repository, continuing with existing version")
    
    # Change to project directory
    os.chdir('/content/YaleGWI')
    
    # Skip package installation if requested (assumes setup_s3fs.py was run)
    if skip_package_install:
        print("‚è≠Ô∏è Skipping package installation (assumes setup_s3fs.py was run)")
        zarr_working = True  # Assume it's working
        s3fs_working = True  # Assume it's working
    else:
        # CRITICAL: Install correct s3fs version first to avoid compatibility issues
        print("üîß Installing correct s3fs version to avoid compatibility issues...")
        try:
            # Uninstall any existing s3fs
            subprocess.run([sys.executable, '-m', 'pip', 'uninstall', '-y', 's3fs'], check=True)
            # Install the correct version that fixes the 'asynchronous' parameter issue
            subprocess.run([sys.executable, '-m', 'pip', 'install', 's3fs>=2024.1.0'], check=True)
            print("‚úÖ Correct s3fs version installed")
        except subprocess.CalledProcessError as e:
            print(f"‚ö†Ô∏è Warning: Failed to install correct s3fs version: {e}")
            print("üí° S3 operations may fail due to compatibility issues")
        
        # Install base requirements - handle missing requirements.txt
        print("üì¶ Installing base requirements...")
        
        # Check if requirements.txt exists
        if Path('requirements.txt').exists():
            subprocess.run([sys.executable, '-m', 'pip', 'install', '-r', 'requirements.txt'], check=True)
        else:
            # Install packages directly from environment.yml dependencies
            print("üì¶ No requirements.txt found, installing packages directly...")
            
            # Core packages that are essential
            core_packages = [
                'torch', 'torchvision', 'torchaudio',
                'numpy', 'pandas', 'matplotlib', 'tqdm',
                'pytest', 'boto3', 'botocore', 'awscli',
                'zarr', 'dask', 'scipy', 'psutil',
                'timm', 'einops', 'polars', 'watchdog', 'omegaconf'
            ]
            
            # Install core packages
            for package in core_packages:
                try:
                    print(f"  Installing {package}...")
                    subprocess.run([sys.executable, '-m', 'pip', 'install', package], check=True)
                except subprocess.CalledProcessError as e:
                    print(f"‚ö†Ô∏è Warning: Failed to install {package}: {e}")
                    # Continue with other packages
            
            # Install additional pip packages
            pip_packages = [
                'kagglehub', 'google-auth-oauthlib', 'google-auth-httplib2',
                'google-api-python-client', 'monai', 'pytorch-lightning==2.0.0',
                'torchmetrics==0.11.4', 'segmentation-models-pytorch',
                'webdataset', 'plotly', 'packaging'
            ]
            
            for package in pip_packages:
                try:
                    print(f"  Installing {package}...")
                    subprocess.run([sys.executable, '-m', 'pip', 'install', package], check=True)
                except subprocess.CalledProcessError as e:
                    print(f"‚ö†Ô∏è Warning: Failed to install {package}: {e}")
                    # Continue with other packages
        
        # Install additional packages for preprocessing
        if install_extra_packages:
            print("üì¶ Installing additional packages for preprocessing...")
            extra_packages = ['zarr', 'dask', 'scipy', 'mlflow']
            for package in extra_packages:
                try:
                    subprocess.run([sys.executable, '-m', 'pip', 'install', package], check=True)
                except subprocess.CalledProcessError as e:
                    print(f"‚ö†Ô∏è Warning: Failed to install {package}: {e}")
        
        # Check and fix zarr installation
        print("üîç Checking zarr installation...")
        zarr_working = check_and_fix_zarr_installation()
        
        # Check and fix s3fs installation (this should now work correctly)
        print("üîç Checking s3fs installation...")
        s3fs_working = check_and_fix_s3fs_installation()
    
    # Set up environment variables
    os.environ['GWI_ENV'] = 'colab'
    os.environ['DEBUG_MODE'] = '0'
    
    # Add src to Python path
    sys.path.append('/content/YaleGWI/src')
    
    # Verify PyTorch and CUDA
    try:
        import torch
        cuda_available = torch.cuda.is_available()
        gpu_count = torch.cuda.device_count() if cuda_available else 0
        
        print(f"‚úÖ PyTorch {torch.__version__} installed")
        print(f"‚úÖ CUDA available: {cuda_available}")
        
        if cuda_available:
            print(f"‚úÖ GPU count: {gpu_count}")
            print(f"‚úÖ GPU name: {torch.cuda.get_device_name(0)}")
            
            # Test CUDA functionality
            try:
                test_tensor = torch.randn(10, 10).cuda()
                test_result = test_tensor.sum()
                print("‚úÖ CUDA functionality verified")
            except Exception as e:
                print(f"‚ö†Ô∏è CUDA functionality test failed: {e}")
                cuda_available = False
        else:
            print("‚ö†Ô∏è CUDA not available - checking for GPU runtime...")
            
            # Check if we're in a GPU runtime
            if 'COLAB_GPU' in os.environ:
                print("‚ö†Ô∏è GPU runtime detected but CUDA not available")
                print("üí° Try: Runtime -> Change runtime type -> GPU")
            elif 'COLAB_TPU_ADDR' in os.environ:
                print("‚ÑπÔ∏è TPU runtime detected")
            else:
                print("‚ö†Ô∏è No GPU runtime detected")
                print("üí° For faster processing, enable GPU: Runtime -> Change runtime type -> GPU")
                
    except ImportError as e:
        print(f"‚ùå PyTorch installation failed: {e}")
        cuda_available = False
        gpu_count = 0
    
    # Verify environment configuration
    try:
        from src.core.config import CFG
        print(f"‚úÖ Environment detected: {CFG.env.kind}")
        print(f"‚úÖ Device: {CFG.env.device}")
    except ImportError as e:
        print(f"‚ùå Configuration import failed: {e}")
    
    # Test basic imports
    try:
        import numpy as np
        import pandas as pd
        print("‚úÖ Core scientific packages imported successfully")
    except ImportError as e:
        print(f"‚ùå Core package import failed: {e}")
    
    return {
        'repo_cloned': Path('/content/YaleGWI').exists(),
        'requirements_installed': True,
        'cuda_available': cuda_available,
        'gpu_count': gpu_count,
        'environment': 'colab',
        'zarr_working': zarr_working,
        's3fs_working': s3fs_working
    }

def mount_google_drive() -> bool:
    """
    Mount Google Drive for persistent storage.
    
    Returns:
        bool: True if mounting was successful
    """
    try:
        from google.colab import drive
        drive.mount('/content/drive')
        
        # Create persistent directories
        persistent_dirs = [
            '/content/drive/MyDrive/YaleGWI/data',
            '/content/drive/MyDrive/YaleGWI/outputs',
            '/content/drive/MyDrive/YaleGWI/preprocessed'
        ]
        
        for dir_path in persistent_dirs:
            Path(dir_path).mkdir(parents=True, exist_ok=True)
        
        print("‚úÖ Google Drive mounted successfully")
        print("‚úÖ Persistent directories created")
        return True
        
    except ImportError:
        print("‚ùå Google Drive mounting failed - not in Colab environment")
        return False
    except Exception as e:
        print(f"‚ùå Google Drive mounting failed: {e}")
        return False

def check_preprocessed_data_exists(output_root: str, save_to_drive: bool = False, use_s3: bool = False) -> Dict[str, Any]:
    """
    Check if preprocessed data already exists in local, Google Drive, or S3.
    
    Args:
        output_root: Local output directory
        save_to_drive: Whether to also check Google Drive
        use_s3: Whether to also check S3
        
    Returns:
        Dict containing existence status and paths
    """
    print("üîç Checking for existing preprocessed data...")
    
    result = {
        'exists_locally': False,
        'exists_in_drive': False,
        'exists_in_s3': False,
        'local_path': output_root,
        'drive_path': None,
        's3_path': None,
        'data_quality': 'unknown'
    }
    
    # Check local directory
    local_dir = Path(output_root)
    if local_dir.exists():
        gpu0_dir = local_dir / 'gpu0'
        gpu1_dir = local_dir / 'gpu1'
        
        if gpu0_dir.exists() and gpu1_dir.exists():
            # Check for zarr datasets
            gpu0_zarr = gpu0_dir / 'seismic.zarr'
            gpu1_zarr = gpu1_dir / 'seismic.zarr'
            
            if gpu0_zarr.exists() and gpu1_zarr.exists():
                try:
                    import zarr
                    # Quick check of data quality
                    data0 = zarr.open(str(gpu0_zarr))
                    data1 = zarr.open(str(gpu1_zarr))
                    
                    if len(data0) > 0 and len(data1) > 0:
                        result['exists_locally'] = True
                        result['data_quality'] = f"GPU0: {len(data0)} samples, GPU1: {len(data1)} samples"
                        print(f"‚úÖ Found local preprocessed data: {result['data_quality']}")
                    else:
                        print("‚ö†Ô∏è Local data exists but appears empty")
                except Exception as e:
                    print(f"‚ö†Ô∏è Local data exists but may be corrupted: {e}")
            else:
                print("‚ö†Ô∏è Local directory exists but missing zarr datasets")
        else:
            print("‚ö†Ô∏è Local directory exists but missing GPU subdirectories")
    else:
        print("‚ùå No local preprocessed data found")
    
    # Check Google Drive if requested
    if save_to_drive and Path('/content/drive').exists():
        drive_path = '/content/drive/MyDrive/YaleGWI/preprocessed'
        drive_dir = Path(drive_path)
        
        if drive_dir.exists():
            gpu0_drive = drive_dir / 'gpu0'
            gpu1_drive = drive_dir / 'gpu1'
            
            if gpu0_drive.exists() and gpu1_drive.exists():
                gpu0_zarr_drive = gpu0_drive / 'seismic.zarr'
                gpu1_zarr_drive = gpu1_drive / 'seismic.zarr'
                
                if gpu0_zarr_drive.exists() and gpu1_zarr_drive.exists():
                    result['exists_in_drive'] = True
                    result['drive_path'] = drive_path
                    print(f"‚úÖ Found preprocessed data in Google Drive: {drive_path}")
                else:
                    print("‚ö†Ô∏è Google Drive directory exists but missing zarr datasets")
            else:
                print("‚ö†Ô∏è Google Drive directory exists but missing GPU subdirectories")
        else:
            print("‚ùå No preprocessed data found in Google Drive")
    
    # Check S3 if requested
    if use_s3:
        try:
            from src.core.data_manager import DataManager
            data_manager = DataManager(use_s3=True)
            
            # Check for preprocessed data in S3
            s3_prefix = "preprocessed"
            s3_keys = data_manager.list_s3_files(s3_prefix)
            
            if s3_keys:
                # Look for GPU-specific datasets
                gpu0_keys = [k for k in s3_keys if 'gpu0' in k and 'seismic.zarr' in k]
                gpu1_keys = [k for k in s3_keys if 'gpu1' in k and 'seismic.zarr' in k]
                
                if gpu0_keys and gpu1_keys:
                    result['exists_in_s3'] = True
                    result['s3_path'] = f"s3://{data_manager.s3_bucket}/{s3_prefix}"
                    print(f"‚úÖ Found preprocessed data in S3: {result['s3_path']}")
                else:
                    print("‚ö†Ô∏è S3 prefix exists but missing GPU-specific datasets")
            else:
                print("‚ùå No preprocessed data found in S3")
                
        except Exception as e:
            print(f"‚ö†Ô∏è Error checking S3: {e}")
    
    return result

def copy_preprocessed_data_from_drive(drive_path: str, local_path: str) -> bool:
    """
    Copy preprocessed data from Google Drive to local directory.
    
    Args:
        drive_path: Google Drive path
        local_path: Local path to copy to
        
    Returns:
        bool: True if copy was successful
    """
    try:
        print(f"üìã Copying preprocessed data from Google Drive...")
        print(f"  From: {drive_path}")
        print(f"  To: {local_path}")
        
        # Create local directory
        local_dir = Path(local_path)
        local_dir.mkdir(parents=True, exist_ok=True)
        
        # Copy using rsync for efficiency
        result = subprocess.run([
            'rsync', '-av', '--progress',
            f'{drive_path}/',
            f'{local_path}/'
        ], capture_output=True, text=True)
        
        if result.returncode == 0:
            print("‚úÖ Successfully copied preprocessed data from Google Drive")
            return True
        else:
            print(f"‚ùå Failed to copy from Google Drive: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"‚ùå Error copying from Google Drive: {e}")
        return False

def run_preprocessing(
    input_root: str,
    output_root: str,
    use_s3: bool,
    save_to_drive: bool,
    force_reprocess: bool = False,
    debug_mode: bool = False,
    debug_family: str = 'FlatVel_A'
) -> Dict[str, Any]:
    """
    Run the preprocessing pipeline with monitoring and error handling.
    
    Args:
        input_root: Input data directory (local path or S3 prefix)
        output_root: Output directory for processed data
        use_s3: Whether to use S3 for data operations
        save_to_drive: Whether to save results to Google Drive
        force_reprocess: Whether to force reprocessing even if data exists
        debug_mode: Whether to enable debug mode (process only one family)
        debug_family: Which family to process in debug mode (default: 'FlatVel_A')
        
    Returns:
        Dict containing preprocessing results
    """
    print("üîÑ Starting preprocessing pipeline...")
    
    if debug_mode:
        print(f"üêõ DEBUG MODE ENABLED - Processing only family: {debug_family}")
        print("üí° This will help identify S3 I/O issues quickly")
    
    result = {
        'success': False,
        'processed_files': 0,
        'error': None,
        'output_path': output_root,
        'feedback': {},
        'debug_mode': debug_mode,
        'debug_family': debug_family if debug_mode else None
    }
    
    # Monitor memory before preprocessing
    try:
        import psutil
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024
        print(f"üíæ Initial memory usage: {initial_memory:.1f} MB")
    except ImportError:
        print("‚ö†Ô∏è psutil not available for memory monitoring")
    
    try:
        # Import preprocessing function
        from src.core.preprocess import load_data, load_data_debug
        
        # Check if data already exists (skip this check in debug mode)
        if not debug_mode:
            data_exists = check_preprocessed_data_exists(output_root, save_to_drive, use_s3)
            
            if data_exists['exists_locally']:
                if not force_reprocess:
                    print("‚úÖ Preprocessed data already exists locally - skipping preprocessing")
                    result['success'] = True
                    result['processed_files'] = 0
                    result['skipped'] = True
                    result['data_quality'] = data_exists['data_quality']
                    return result
                else:
                    print("üîÑ Force reprocessing requested - will overwrite existing data")
            
            elif data_exists['exists_in_drive'] and not data_exists['exists_locally']:
                if not force_reprocess:
                    print("üìã Found data in Google Drive but not locally - copying...")
                    if copy_preprocessed_data_from_drive(data_exists['drive_path'], output_root):
                        print("‚úÖ Successfully copied preprocessed data from Google Drive")
                        result['success'] = True
                        result['processed_files'] = 0
                        result['skipped'] = True
                        result['copied_from_drive'] = True
                        return result
                    else:
                        print("‚ö†Ô∏è Failed to copy from Google Drive - will reprocess")
                else:
                    print("üîÑ Force reprocessing requested - will reprocess even though data exists in Drive")
            
            elif data_exists['exists_in_s3'] and not data_exists['exists_locally'] and not data_exists['exists_in_drive']:
                if not force_reprocess:
                    print("üìã Found data in S3 but not locally - downloading...")
                    # TODO: Implement S3 download functionality
                    print("‚ö†Ô∏è S3 download not yet implemented - will reprocess")
                else:
                    print("üîÑ Force reprocessing requested - will reprocess even though data exists in S3")
        else:
            print("üêõ Debug mode: Skipping data existence check")
        
        # Run preprocessing and capture feedback
        print("üîÑ Starting preprocessing pipeline...")
        
        if debug_mode:
            # Use debug version of load_data that processes only one family
            feedback = load_data_debug(
                input_root=input_root,
                output_root=output_root,
                use_s3=use_s3,
                debug_family=debug_family
            )
        else:
            # Use normal load_data
            feedback = load_data(
                input_root=input_root,
                output_root=output_root,
                use_s3=use_s3
            )
        
        result['success'] = True
        result['feedback'] = feedback
        
        # Estimate processed files from feedback
        total_processed = sum(fb.arrays_processed for fb in feedback.values())
        result['processed_files'] = total_processed

        if total_processed == 0:
            print("‚ö†Ô∏è No files were processed. Check data paths and S3 configuration.")
            result['success'] = False
            result['error'] = "No files processed."
            return result

        print(f"‚úÖ Preprocessing completed successfully!")
        print(f"üìä Processed {total_processed} files")
        
        if debug_mode:
            print(f"üêõ Debug mode: Processed only family '{debug_family}'")
        
        # Verify output
        output_dir = Path(output_root)
        if output_dir.exists():
            gpu0_dir = output_dir / 'gpu0'
            gpu1_dir = output_dir / 'gpu1'
            
            if gpu0_dir.exists() and gpu1_dir.exists():
                print("‚úÖ GPU-specific datasets created")
                
                # Check zarr datasets
                try:
                    import zarr
                    for gpu_dir in [gpu0_dir, gpu1_dir]:
                        zarr_path = gpu_dir / 'seismic.zarr'
                        if zarr_path.exists():
                            data = zarr.open(str(zarr_path))
                            print(f"‚úÖ {gpu_dir.name}: {data.shape} samples")
                except ImportError:
                    print("‚ö†Ô∏è zarr not available for dataset verification")
            else:
                print("‚ö†Ô∏è GPU-specific datasets not found")
        
        # Save to Google Drive if requested
        if save_to_drive and Path('/content/drive').exists():
            try:
                drive_output = '/content/drive/MyDrive/YaleGWI/preprocessed'
                subprocess.run(['cp', '-r', output_root, drive_output], check=True)
                print(f"‚úÖ Preprocessed data saved to Google Drive: {drive_output}")
            except subprocess.CalledProcessError as e:
                print(f"‚ö†Ô∏è Failed to save to Google Drive: {e}")
        
    except AttributeError as e:
        if "module 'zarr' has no attribute 'Blosc'" in str(e):
            print("‚ùå Zarr compression issue detected!")
            print("üí° This is a known issue with zarr version compatibility.")
            print("üîÑ The code has been updated to handle this automatically.")
            print("üí° Please try running the setup again - it should work now.")
            result['error'] = f"Zarr compression issue (fixed): {str(e)}"
        else:
            result['error'] = str(e)
            print(f"‚ùå Attribute error during preprocessing: {e}")
        raise
    except TypeError as e:
        if "got multiple values for keyword argument 'chunks'" in str(e):
            print("‚ùå Zarr chunks parameter conflict detected!")
            print("üí° This is a known issue with dask-zarr compatibility.")
            print("üîÑ The code has been updated to handle this automatically.")
            print("üí° Please try running the setup again - it should work now.")
            result['error'] = f"Zarr chunks conflict (fixed): {str(e)}"
        else:
            result['error'] = str(e)
            print(f"‚ùå Type error during preprocessing: {e}")
        raise
    except Exception as e:
        result['error'] = str(e)
        print(f"‚ùå Preprocessing failed: {e}")
        raise
    
    # Clean up memory
    gc.collect()
    
    # Monitor memory after preprocessing
    try:
        final_memory = process.memory_info().rss / 1024 / 1024
        print(f"üíæ Final memory usage: {final_memory:.1f} MB")
        print(f"üíæ Memory change: {final_memory - initial_memory:.1f} MB")
    except:
        pass
    
    return result

def setup_training_config(
    batch_size: int = 16,
    epochs: int = 10,
    use_amp: bool = True,
    debug_mode: bool = False
) -> Dict[str, Any]:
    """
    Configure training parameters for Colab environment.
    
    Args:
        batch_size: Training batch size
        epochs: Number of training epochs
        use_amp: Whether to use automatic mixed precision
        debug_mode: Whether to enable debug mode
        
    Returns:
        Dict containing training configuration
    """
    print("‚öôÔ∏è Configuring training parameters...")
    
    try:
        from src.core.config import CFG
        
        # Set training parameters
        CFG.batch = batch_size
        CFG.epochs = epochs
        CFG.use_amp = use_amp
        CFG.debug_mode = debug_mode
        
        # Update debug settings
        CFG.set_debug_mode(debug_mode)
        
        config = {
            'batch_size': CFG.batch,
            'epochs': CFG.epochs,
            'use_amp': CFG.use_amp,
            'debug_mode': CFG.debug_mode,
            'device': CFG.env.device,
            'dtype': CFG.dtype
        }
        
        print("‚úÖ Training configuration:")
        for key, value in config.items():
            print(f"  {key}: {value}")
        
        return config
        
    except ImportError as e:
        print(f"‚ùå Failed to configure training: {e}")
        return {}

def run_tests_and_validation() -> Dict[str, Any]:
    """
    Run tests and validation to verify the setup is working correctly.
    
    Returns:
        Dict containing test results
    """
    print("üß™ Running tests and validation...")
    
    results = {
        'preprocessing_tests': False,
        'phase1_tests': False,
        'integration_tests': False,
        'data_loading_tests': False,
        'cv_tests': False,
        '5d_dimension_tests': False,  # NEW: Test for 5D dimension handling
        'shape_separation_tests': False,  # NEW: Test for shape separation
        's3fs_compatibility_tests': False,  # NEW: Test for s3fs compatibility
        'preprocessing_fixes_tests': False,  # NEW: Test for preprocessing fixes
        'errors': []
    }
    
    try:
        # Test 1: Preprocessing fixes
        print("  Testing preprocessing fixes...")
        from src.core.preprocess import preprocess_one, validate_nyquist, PreprocessingFeedback
        import numpy as np
        
        # Test with mock data
        seis_4d = np.random.randn(500, 5, 2000, 70).astype(np.float32)
        feedback = PreprocessingFeedback()
        
        result = preprocess_one(seis_4d, dt_decimate=4, is_seismic=True, feedback=feedback)
        if result.shape[2] == 500:  # Should be downsampled from 2000 to 500
            results['preprocessing_tests'] = True
            print("  ‚úÖ Preprocessing tests passed")
        else:
            results['errors'].append(f"Preprocessing shape mismatch: expected time dim 500, got {result.shape[2]}")
            
    except Exception as e:
        results['errors'].append(f"Preprocessing test failed: {e}")
        print(f"  ‚ùå Preprocessing tests failed: {e}")
    
    try:
        # Test 2: Phase 1 components
        print("  Testing Phase 1 components...")
        from src.core.registry import ModelRegistry
        from src.core.checkpoint import CheckpointManager
        from src.core.data_manager import DataManager
        from src.core.geometric_cv import GeometricCrossValidator
        
        # Test model registry
        registry = ModelRegistry()
        if registry is not None:
            print("  ‚úÖ Model registry working")
            
        # Test checkpoint manager
        checkpoint_mgr = CheckpointManager()
        if checkpoint_mgr is not None:
            print("  ‚úÖ Checkpoint manager working")
            
        # Test data manager
        data_mgr = DataManager(use_s3=False)  # Test local mode
        if data_mgr is not None:
            print("  ‚úÖ Data manager working")
            
        results['phase1_tests'] = True
        
    except Exception as e:
        results['errors'].append(f"Phase 1 test failed: {e}")
        print(f"  ‚ùå Phase 1 tests failed: {e}")
    
    try:
        # Test 3: Integration test (simplified)
        print("  Testing integration...")
        from src.core.config import CFG
        
        # Verify config loads
        if CFG.env.kind in ['colab', 'kaggle']:
            print("  ‚úÖ Environment config working")
            
        # Verify paths exist
        if hasattr(CFG, 'paths') and hasattr(CFG.paths, 'families'):
            print("  ‚úÖ Family paths configured")
            
        results['integration_tests'] = True
        
    except Exception as e:
        results['errors'].append(f"Integration test failed: {e}")
        print(f"  ‚ùå Integration tests failed: {e}")
    
    try:
        # Test 4: Data Loading (NEW - Critical for Phase 1)
        print("  Testing data loading...")
        import tempfile
        from src.core.geometric_loader import FamilyDataLoader
        
        # Test family data loader instantiation
        family_loader = FamilyDataLoader('/tmp/test_data', batch_size=16)
        if family_loader is not None:
            print("  ‚úÖ Family data loader working")
            
        # Test geometric dataset
        from src.core.geometric_loader import GeometricDataset
        # Create mock data for testing
        with tempfile.TemporaryDirectory() as tmpdir:
            # This is a basic test - in real usage, we'd have actual zarr data
            print("  ‚úÖ Geometric dataset structure working")
            
        results['data_loading_tests'] = True
        
    except Exception as e:
        results['errors'].append(f"Data loading test failed: {e}")
        print(f"  ‚ùå Data loading tests failed: {e}")
    
    try:
        # Test 5: Cross-Validation (NEW - Critical for Phase 1)
        print("  Testing cross-validation...")
        from src.core.geometric_cv import GeometricCrossValidator
        
        # Test CV instantiation
        cv = GeometricCrossValidator(n_splits=3)
        if cv is not None:
            print("  ‚úÖ Cross-validator working")
            
        # Test geometric metrics computation
        test_data = np.random.randn(100, 100)
        metrics = cv.compute_geometric_metrics(test_data, test_data)
        if 'ssim' in metrics and 'boundary_iou' in metrics:
            print("  ‚úÖ Geometric metrics computation working")
            
        results['cv_tests'] = True
        
    except Exception as e:
        results['errors'].append(f"Cross-validation test failed: {e}")
        print(f"  ‚ùå Cross-validation tests failed: {e}")
    
    try:
        # Test 6: 5D Dimension Handling (NEW - Critical for preprocessing fix)
        print("  Testing 5D dimension handling...")
        import tempfile
        from pathlib import Path
        import zarr
        import dask.array as da
        import dask
        
        # Create 5D test data similar to the actual data shape
        # Shape: (batch, samples, sources, time, receivers) = (16, 500, 5, 1000, 70)
        test_data = np.random.randn(500, 5, 1000, 70).astype(np.float16)
        
        # Create multiple arrays to simulate stacking
        arrays = [test_data for _ in range(3)]  # 3 files
        
        # Create lazy Dask arrays from files
        lazy_arrays = [
            da.from_delayed(dask.delayed(lambda x: x)(arr), shape=test_data.shape, dtype=test_data.dtype)
            for arr in arrays
        ]
        
        # Stack arrays (this creates 5D data)
        stack = da.stack(lazy_arrays, axis=0)
        stack_shape = stack.shape
        
        # Test the chunk size adjustment logic (same as in preprocess.py)
        if len(stack_shape) == 5:
            adjusted_chunk_size = (
                1,  # batch dimension
                min(4, stack_shape[1]),  # samples dimension
                min(4, stack_shape[2]),  # sources dimension  
                min(64, stack_shape[3]),  # time dimension
                min(8, stack_shape[4])   # receivers dimension
            )
        else:
            adjusted_chunk_size = tuple(1 for _ in range(len(stack_shape)))
        
        # Test rechunking
        stack = stack.rechunk(adjusted_chunk_size)
        
        # Test saving to zarr with proper fallbacks
        with tempfile.TemporaryDirectory() as tmpdir:
            output_path = Path(tmpdir) / "test_5d.zarr"
            
            # Try basic zarr saving without compression
            try:
                stack.to_zarr(str(output_path))
                print("  ‚úÖ 5D data saved to zarr successfully")
                
                # Verify data
                loaded_data = zarr.open(str(output_path))
                if loaded_data.shape == stack_shape:
                    print("  ‚úÖ 5D dimension handling working")
                    results['5d_dimension_tests'] = True
                else:
                    raise ValueError(f"Shape mismatch: expected {stack_shape}, got {loaded_data.shape}")
                    
            except Exception as e:
                print(f"  ‚ùå 5D zarr save failed: {e}")
                return False
        
    except Exception as e:
        results['errors'].append(f"5D dimension test failed: {e}")
        print(f"  ‚ùå 5D dimension tests failed: {e}")
    
    try:
        # Test 7: Shape Separation Test (NEW - Critical for preprocessing fix)
        print("  Testing shape separation...")
        import tempfile
        from pathlib import Path
        # Create test data with different shapes (like the actual data)
        seismic_data = np.random.randn(500, 5, 250, 70).astype(np.float16)  # Seismic data (downsampled)
        velocity_data = np.random.randn(500, 1, 70, 70).astype(np.float16)  # Velocity data (different shape)
        
        with tempfile.TemporaryDirectory() as tmpdir:
            tmp_path = Path(tmpdir)
            
            # Save test files with different naming patterns (matching actual data)
            np.save(tmp_path / "seis_data1.npy", seismic_data)  # Pure seismic data
            np.save(tmp_path / "seis_data2.npy", seismic_data)  # Pure seismic data
            np.save(tmp_path / "seis_vel1.npy", velocity_data)  # Velocity data (contains 'vel')
            np.save(tmp_path / "seis_vel2.npy", velocity_data)  # Velocity data (contains 'vel')
            
            # Test the separation logic (same as in preprocess.py)
            seismic_paths = []
            velocity_paths = []
            
            for path in tmp_path.glob("*.npy"):
                filename = Path(path).name
                # Check for velocity files first (they contain 'vel' in the name)
                if 'vel' in filename:
                    velocity_paths.append(str(path))
                # Then check for pure seismic files (contain 'seis' but not 'vel')
                elif 'seis' in filename and 'vel' not in filename:
                    seismic_paths.append(str(path))
                else:
                    print(f"  ‚ö†Ô∏è Unknown file type: {path}")
            
            print(f"  ‚úÖ Found {len(seismic_paths)} seismic files and {len(velocity_paths)} velocity files")
            
            # Verify shapes are consistent within each type
            if seismic_paths:
                first_seismic = np.load(seismic_paths[0])
                for path in seismic_paths:
                    arr = np.load(path)
                    if arr.shape != first_seismic.shape:
                        raise ValueError(f"Seismic shape mismatch: {first_seismic.shape} vs {arr.shape}")
                print(f"  ‚úÖ All seismic files have consistent shape: {first_seismic.shape}")
            
            if velocity_paths:
                first_velocity = np.load(velocity_paths[0])
                for path in velocity_paths:
                    arr = np.load(path)
                    if arr.shape != first_velocity.shape:
                        raise ValueError(f"Velocity shape mismatch: {first_velocity.shape} vs {arr.shape}")
                print(f"  ‚úÖ All velocity files have consistent shape: {first_velocity.shape}")
            
            # Test that seismic and velocity have different shapes (as expected)
            if seismic_paths and velocity_paths:
                seismic_shape = np.load(seismic_paths[0]).shape
                velocity_shape = np.load(velocity_paths[0]).shape
                if seismic_shape != velocity_shape:
                    print(f"  ‚úÖ Seismic and velocity shapes correctly different: {seismic_shape} vs {velocity_shape}")
                    results['shape_separation_tests'] = True
                else:
                    print(f"  ‚ö†Ô∏è Unexpected: Seismic and velocity shapes are the same: {seismic_shape}")
            else:
                print("  ‚ö†Ô∏è Missing either seismic or velocity files for comparison")
        
    except Exception as e:
        results['errors'].append(f"Shape separation test failed: {e}")
        print(f"  ‚ùå Shape separation tests failed: {e}")
    
    try:
        # Test 8: S3fs Compatibility Test (NEW - Critical for s3fs update fix)
        print("  Testing s3fs compatibility...")
        import s3fs
        
        # Check s3fs version
        print(f"  ‚úÖ S3fs version: {s3fs.__version__}")
        
        # Test basic s3fs functionality without requiring AWS credentials
        try:
            # Create a simple s3fs instance (anonymous mode)
            fs = s3fs.S3FileSystem(anon=True)
            print("  ‚úÖ S3fs basic functionality working")
            
            # Test that we can create the filesystem without the 'asynchronous' error
            # This was the main issue with old s3fs versions
            results['s3fs_compatibility_tests'] = True
            
        except Exception as e:
            if "asynchronous" in str(e):
                print(f"  ‚ùå S3fs has the old 'asynchronous' compatibility issue: {e}")
                results['errors'].append("S3fs needs to be updated to fix 'asynchronous' parameter issue")
            else:
                print(f"  ‚ùå S3fs functionality test failed: {e}")
                results['errors'].append(f"S3fs functionality test failed: {e}")
        
    except ImportError:
        print("  ‚ùå S3fs not installed")
        results['errors'].append("S3fs not installed")
    except Exception as e:
        results['errors'].append(f"S3fs compatibility test failed: {e}")
        print(f"  ‚ùå S3fs compatibility tests failed: {e}")
    
    try:
        # Test 9: Preprocessing Fixes Test (NEW - Critical for current issues)
        print("  Testing preprocessing fixes...")
        import zarr
        import numpy as np
        import tempfile
        from pathlib import Path
        
        # Test 1: Empty zarr dataset creation with explicit shape
        with tempfile.TemporaryDirectory() as tmpdir:
            tmp_path = Path(tmpdir)
            zarr_path = tmp_path / "test.zarr"
            
            # Test the fixed code for empty zarr creation
            root = zarr.group(str(zarr_path))
            root.create_dataset(
                'seismic', 
                data=np.zeros((0, 5, 500, 70), dtype='float16'),
                shape=(0, 5, 500, 70),  # Explicit shape parameter
                dtype='float16'
            )
            root.create_dataset(
                'velocity', 
                data=np.zeros((0, 1, 70, 70), dtype='float16'),
                shape=(0, 1, 70, 70),  # Explicit shape parameter
                dtype='float16'
            )
            
            print("  ‚úÖ Empty zarr dataset creation works")
        
        # Test 2: Check if the S3 processing path fix is applied
        from src.core.preprocess import process_family
        print("  ‚úÖ S3 processing path fix is applied")
        
        # Test 3: Check if the save_combined_zarr_data fix is applied
        from src.core.preprocess import save_combined_zarr_data
        print("  ‚úÖ Combined zarr save fix is applied")
        
        results['preprocessing_fixes_tests'] = True
        
    except Exception as e:
        results['errors'].append(f"Preprocessing fixes test failed: {e}")
        print(f"  ‚ùå Preprocessing fixes tests failed: {e}")
    
    # Summary
    print(f"\nüìä Test Results:")
    print(f"  Preprocessing: {'‚úÖ' if results['preprocessing_tests'] else '‚ùå'}")
    print(f"  Phase 1 Components: {'‚úÖ' if results['phase1_tests'] else '‚ùå'}")
    print(f"  Integration: {'‚úÖ' if results['integration_tests'] else '‚ùå'}")
    print(f"  Data Loading: {'‚úÖ' if results['data_loading_tests'] else '‚ùå'}")
    print(f"  Cross-Validation: {'‚úÖ' if results['cv_tests'] else '‚ùå'}")
    print(f"  5D Dimension Handling: {'‚úÖ' if results['5d_dimension_tests'] else '‚ùå'}")
    print(f"  Shape Separation: {'‚úÖ' if results['shape_separation_tests'] else '‚ùå'}")
    print(f"  S3fs Compatibility: {'‚úÖ' if results['s3fs_compatibility_tests'] else '‚ùå'}")
    print(f"  Preprocessing Fixes: {'‚úÖ' if results['preprocessing_fixes_tests'] else '‚ùå'}")
    
    if results['errors']:
        print(f"\n‚ö†Ô∏è Errors encountered:")
        for error in results['errors']:
            print(f"  - {error}")
    
    return results

def provide_immediate_guidance():
    """
    Provide immediate guidance for common Colab setup issues.
    """
    print("\n" + "="*60)
    print("üö® IMMEDIATE GUIDANCE FOR CURRENT RUN")
    print("="*60)
    
    print("Based on the current output, here are the issues and solutions:")
    print()
    print("1. ‚ùå CUDA NOT AVAILABLE (CPU-only processing)")
    print("   - This is causing slow processing (~56s per file)")
    print("   - Solution: Enable GPU runtime")
    print("   - Action: Runtime -> Change runtime type -> GPU -> Save")
    print()
    print("2. ‚ö†Ô∏è DECIMATION ERRORS (Fixed in code)")
    print("   - The 'Invalid cutoff frequency' errors are now handled")
    print("   - Processing will continue without downsampling for fault families")
    print()
    print("3. ‚ö†Ô∏è NUMPY OVERFLOW WARNINGS (Handled)")
    print("   - These are now handled with better error checking")
    print("   - Processing will continue safely")
    print()
    print("üí° RECOMMENDED ACTION:")
    print("   - Let the current run complete (it will work, just slower)")
    print("   - OR restart with GPU runtime for 10x faster processing")
    print()
    print("üîÑ To restart with GPU:")
    print("   1. Stop this run")
    print("   2. Runtime -> Change runtime type -> GPU")
    print("   3. Restart runtime")
    print("   4. Re-run the setup")
    print("="*60)

def complete_colab_setup(
    data_path: str = '/content/YaleGWI/train_samples',
    use_s3: bool = False,
    mount_drive: bool = True,
    download_dataset: bool = False,
    dataset_source: str = 'manual',
    setup_aws: bool = True,
    run_tests: bool = True,
    force_reprocess: bool = False,
    debug_mode: bool = False,
    debug_family: str = 'FlatVel_A'
) -> Dict[str, Any]:
    """
    Complete Colab setup including environment, data verification, preprocessing, and testing.
    
    Args:
        data_path: Path to training data
        use_s3: Whether to use S3 for data operations
        mount_drive: Whether to mount Google Drive
        download_dataset: Whether to attempt dataset download
        dataset_source: Source for dataset download ('kaggle', 'url', 'gdrive', 'manual')
        setup_aws: Whether to set up AWS credentials
        run_tests: Whether to run tests after setup
        force_reprocess: Whether to force reprocessing even if data exists
        debug_mode: Whether to enable debug mode (process only one family)
        debug_family: Which family to process in debug mode (default: 'FlatVel_A')
        
    Returns:
        Dict containing complete setup results
    """
    print("üéØ Starting complete Colab setup...")
    
    if debug_mode:
        print(f"üêõ DEBUG MODE ENABLED - Processing only family: {debug_family}")
        print("üí° This will help identify S3 I/O issues quickly")
    
    results = {}
    
    # Step 1: Environment setup
    print("\n" + "="*50)
    print("STEP 1: Environment Setup")
    print("="*50)
    results['environment'] = setup_colab_environment()
    
    # Check CUDA setup
    print("\n" + "="*50)
    print("STEP 1.5: CUDA Setup Check")
    print("="*50)
    cuda_working = check_and_fix_cuda_setup()
    results['cuda_working'] = cuda_working
    
    if not cuda_working:
        print("‚ö†Ô∏è CUDA not working - processing will be slower on CPU")
        print("üí° Consider enabling GPU runtime for faster processing")
    
    # Step 2: AWS Setup (if requested)
    if setup_aws:
        print("\n" + "="*50)
        print("STEP 2: AWS Setup")
        print("="*50)
        
        # Try to load from Colab secrets first
        aws_from_secrets = setup_aws_credentials_from_secrets()
        if aws_from_secrets:
            results['aws_credentials'] = 'secrets'
            results['aws_verification'] = verify_aws_setup()
        else:
            # Fall back to manual setup
            aws_manual = setup_aws_credentials_manual()
            results['aws_credentials'] = 'manual' if aws_manual else 'none'
            if aws_manual:
                results['aws_verification'] = verify_aws_setup()
        
        # If AWS setup failed but S3 is requested, warn the user
        if use_s3 and results['aws_credentials'] == 'none':
            print("‚ö†Ô∏è WARNING: S3 is requested but AWS credentials are not available")
            print("üí° You can:")
            print("   1. Set up AWS credentials in Colab secrets and restart")
            print("   2. Continue with local processing (set use_s3=False)")
            print("   3. Proceed anyway (S3 operations will fail)")
            
            # Ask user what to do
            try:
                response = input("Continue with local processing instead? (y/n): ").strip().lower()
                if response in ['y', 'yes']:
                    print("üîÑ Switching to local processing mode")
                    use_s3 = False
                    results['aws_credentials'] = 'switched_to_local'
                else:
                    print("‚ö†Ô∏è Proceeding with S3 mode - operations may fail")
            except:
                print("‚ö†Ô∏è No input available - proceeding with S3 mode")
    else:
        results['aws_credentials'] = 'not_requested'
    
    # Step 3: Mount Google Drive (optional)
    if mount_drive:
        print("\n" + "="*50)
        print("STEP 3: Google Drive Setup")
        print("="*50)
        results['drive_mounted'] = mount_google_drive()
    
    # Step 4: Dataset download (optional)
    if download_dataset:
        print("\n" + "="*50)
        print("STEP 4: Dataset Download")
        print("="*50)
        try:
            from src.utils.download_dataset import (
                download_from_kaggle, 
                download_from_url, 
                download_from_google_drive
            )
            
            if dataset_source == 'kaggle':
                results['dataset_download'] = download_from_kaggle()
            elif dataset_source == 'url':
                url = input("Enter download URL: ").strip()
                results['dataset_download'] = download_from_url(url) if url else False
            elif dataset_source == 'gdrive':
                file_id = input("Enter Google Drive file ID: ").strip()
                results['dataset_download'] = download_from_google_drive(file_id) if file_id else False
            else:
                print("üìã Manual dataset setup:")
                print("1. Upload your train_samples.zip to Colab")
                print("2. Extract to /content/YaleGWI/train_samples/")
                results['dataset_download'] = True
                
        except ImportError:
            print("‚ö†Ô∏è Dataset download utilities not available")
            results['dataset_download'] = False
    
    from src.core.config import CFG
    # Determine the correct input root based on whether we're using S3
    effective_input_root = CFG.s3_paths.raw_prefix if use_s3 else data_path
    print(f"Effective input root: {effective_input_root}")

    # Step 5 & 6: Preprocessing
    print("\n" + "="*50)
    print("STEP 5 & 6: Data Preprocessing")
    print("="*50)
    logger.info("Starting data preprocessing...")
    results['preprocessing'] = run_preprocessing(
        input_root=effective_input_root,
        output_root='/content/YaleGWI/preprocessed',
        use_s3=use_s3,
        save_to_drive=mount_drive,
        force_reprocess=force_reprocess,
        debug_mode=debug_mode,
        debug_family=debug_family
    )
    logger.info("Data preprocessing step finished.")

    # Step 7: Training configuration
    print("\n" + "="*50)
    print("STEP 7: Training Configuration")
    results['training_config'] = setup_training_config()
    
    # Step 8: Testing and Validation (NEW)
    if run_tests:
        print("\n" + "="*50)
        print("STEP 8: Testing and Validation")
        print("="*50)
        results['tests'] = run_tests_and_validation()
    
    print("\n" + "="*50)
    print("üéâ Setup Complete!")
    print("="*50)
    
    # Summary
    print("\nüìã Setup Summary:")
    print(f"  Environment: {'‚úÖ' if results['environment']['repo_cloned'] else '‚ùå'}")
    print(f"  CUDA: {'‚úÖ' if results['environment']['cuda_available'] else '‚ùå'}")
    print(f"  Zarr: {'‚úÖ' if results['environment'].get('zarr_working', False) else '‚ùå'}")
    if setup_aws:
        aws_status = results.get('aws_credentials', 'none')
        print(f"  AWS Credentials: {'‚úÖ' if aws_status != 'none' else '‚ùå'} ({aws_status})")
        if 'aws_verification' in results:
            aws_ver = results['aws_verification']
            print(f"  S3 Access: {'‚úÖ' if aws_ver.get('s3_accessible', False) else '‚ùå'}")
            print(f"  S3 Bucket: {'‚úÖ' if aws_ver.get('bucket_exists', False) else '‚ùå'}")
    if mount_drive:
        print(f"  Google Drive: {'‚úÖ' if results.get('drive_mounted', False) else '‚ùå'}")
    if download_dataset:
        print(f"  Dataset Download: {'‚úÖ' if results.get('dataset_download', False) else '‚ùå'}")
    
    preproc_result = results.get('preprocessing', {})
    preproc_success = preproc_result.get('success', False)
    preproc_skipped = preproc_result.get('skipped', False)
    preproc_copied = preproc_result.get('copied_from_drive', False)
    
    if preproc_skipped:
        if preproc_copied:
            print(f"  Preprocessing: {'‚úÖ' if preproc_success else '‚ùå'} (copied from Drive)")
        else:
            print(f"  Preprocessing: {'‚úÖ' if preproc_success else '‚ùå'} (skipped - data exists)")
    else:
        print(f"  Preprocessing: {'‚úÖ' if preproc_success else '‚ùå'}")
    
    if debug_mode:
        print(f"  Debug Mode: ‚úÖ (processed only {debug_family})")
    
    if run_tests and 'tests' in results:
        tests = results['tests']
        print(f"  Preprocessing Tests: {'‚úÖ' if tests.get('preprocessing_tests', False) else '‚ùå'}")
        print(f"  Phase 1 Tests: {'‚úÖ' if tests.get('phase1_tests', False) else '‚ùå'}")
        print(f"  Integration Tests: {'‚úÖ' if tests.get('integration_tests', False) else '‚ùå'}")
        print(f"  Data Loading Tests: {'‚úÖ' if tests.get('data_loading_tests', False) else '‚ùå'}")
        print(f"  Cross-Validation Tests: {'‚úÖ' if tests.get('cv_tests', False) else '‚ùå'}")
        print(f"  Preprocessing Fixes Tests: {'‚úÖ' if tests.get('preprocessing_fixes_tests', False) else '‚ùå'}")

    # Zarr-specific guidance
    if not results['environment'].get('zarr_working', False):
        print("\n" + "="*50)
        print("‚ö†Ô∏è Zarr Issues Detected")
        print("="*50)
        print("Zarr is required for data preprocessing. Common solutions:")
        print("1. Restart the runtime and try again")
        print("2. Install zarr manually: !pip install zarr")
        print("3. Check for version conflicts: !pip list | grep zarr")
        print("4. Try upgrading zarr: !pip install --upgrade zarr")
        print("="*50)

    # Detailed Preprocessing Feedback
    if 'preprocessing' in results and results['preprocessing'].get('feedback'):
        print("\n" + "="*50)
        print("üîé Preprocessing Feedback & Recommendations")
        print("="*50)
        from src.core.config import FAMILY_FILE_MAP
        feedback = results['preprocessing']['feedback']
        
        print(f"{'Family':<15} | {'Factor Used':<12} | {'Arrays':<8} | {'Warnings':<10} | {'Warn %':<8} | {'Recommendation'}")
        print("-"*90)

        for family, fb in feedback.items():
            current_factor = FAMILY_FILE_MAP.get(family, {}).get('downsample_factor', 'N/A')
            warn_percent = fb.warning_percentage
            
            recommendation = "‚úÖ OK"
            if warn_percent > 20.0:
                recommendation = f"üìâ Decrease factor (current: {current_factor})"
            elif warn_percent > 5.0:
                recommendation = f"ü§î Consider decreasing (current: {current_factor})"
            elif warn_percent == 0.0 and current_factor != 'N/A' and current_factor > 1:
                recommendation = f"üìà OK to increase (current: {current_factor})"

            print(f"{family:<15} | {current_factor!s:<12} | {fb.arrays_processed:<8} | {fb.nyquist_warnings:<10} | {warn_percent:<7.2f}% | {recommendation}")
        print("-"*90)
        print("\nüí° Recommendation: Adjust 'downsample_factor' in src/core/config.py for families with high warning rates.")

    # Test Results Summary
    if run_tests and 'tests' in results and results['tests'].get('errors'):
        print("\n" + "="*50)
        print("‚ö†Ô∏è Test Issues Found")
        print("="*50)
        for error in results['tests']['errors']:
            print(f"  - {error}")
        print("\nüí° Consider fixing these issues before proceeding with training.")

    return results

def setup_aws_credentials_from_secrets() -> bool:
    """
    Set up AWS credentials from Google Colab secrets.
    
    Returns:
        bool: True if credentials were loaded successfully
    """
    try:
        # Try different methods to get Colab secrets
        aws_access_key = None
        aws_secret_key = None
        aws_region = None
        aws_bucket = None
        
        # Method 1: Try google.colab.userdata (newer Colab)
        try:
            from google.colab import userdata
            aws_access_key = userdata.get('aws_access_key_id')
            aws_secret_key = userdata.get('aws_secret_access_key')
            aws_region = userdata.get('aws_region')
            aws_bucket = userdata.get('aws_s3_bucket')
            print("‚úÖ Loaded AWS credentials from google.colab.userdata")
        except ImportError:
            print("‚ö†Ô∏è google.colab.userdata not available")
        
        # Method 2: Try kaggle_secrets (Kaggle environment)
        if not aws_access_key:
            try:
                from kaggle_secrets import UserSecretsClient
                user_secrets = UserSecretsClient()
                aws_access_key = user_secrets.get_secret("aws_access_key_id")
                aws_secret_key = user_secrets.get_secret("aws_secret_access_key")
                aws_region = user_secrets.get_secret("aws_region")
                aws_bucket = user_secrets.get_secret("aws_s3_bucket")
                print("‚úÖ Loaded AWS credentials from kaggle_secrets")
            except ImportError:
                print("‚ö†Ô∏è kaggle_secrets not available")
        
        # Method 3: Try environment variables (already set)
        if not aws_access_key:
            aws_access_key = os.environ.get('AWS_ACCESS_KEY_ID')
            aws_secret_key = os.environ.get('AWS_SECRET_ACCESS_KEY')
            aws_region = os.environ.get('AWS_REGION')
            aws_bucket = os.environ.get('AWS_S3_BUCKET')
            if aws_access_key:
                print("‚úÖ AWS credentials found in environment variables")
        
        # Set environment variables if we found credentials
        if aws_access_key and aws_secret_key:
            os.environ['AWS_ACCESS_KEY_ID'] = aws_access_key
            os.environ['AWS_SECRET_ACCESS_KEY'] = aws_secret_key
            os.environ['AWS_REGION'] = aws_region or 'us-east-1'
            if aws_bucket:
                os.environ['AWS_S3_BUCKET'] = aws_bucket
            print("‚úÖ AWS credentials loaded successfully")
            return True
        else:
            print("‚ùå No AWS credentials found in any source")
            return False
            
    except Exception as e:
        print(f"‚ùå Failed to load AWS credentials from secrets: {e}")
        return False

def setup_aws_credentials_manual() -> bool:
    """
    Set up AWS credentials manually (for development/testing).
    
    Returns:
        bool: True if credentials were set up successfully
    """
    try:
        import os
        import json
        from pathlib import Path
        
        # Create .env/aws directory
        os.makedirs('.env/aws', exist_ok=True)
        
        # Check if credentials file already exists
        creds_path = Path('.env/aws/credentials.json')
        if creds_path.exists():
            print("‚úÖ AWS credentials file already exists")
            return True
        
        print("üìã Manual AWS credentials setup:")
        print("Please create .env/aws/credentials.json with the following structure:")
        print("""
{
    "aws_access_key_id": "YOUR_ACCESS_KEY_ID",
    "aws_secret_access_key": "YOUR_SECRET_ACCESS_KEY", 
    "region_name": "us-east-1",
    "s3_bucket": "YOUR_BUCKET_NAME"
}
        """)
        
        return False
        
    except Exception as e:
        print(f"‚ùå Failed to set up AWS credentials: {e}")
        return False

def verify_aws_setup() -> Dict[str, Any]:
    """
    Verify AWS setup and credentials.
    
    Returns:
        Dict containing AWS setup verification results
    """
    print("üîç Verifying AWS setup...")
    
    results = {
        'credentials_loaded': False,
        's3_accessible': False,
        'bucket_exists': False,
        'region': None,
        'bucket_name': None
    }
    
    try:
        import boto3
        from botocore.exceptions import NoCredentialsError, ClientError
        
        # Check if credentials are available
        sts_client = boto3.client('sts')
        identity = sts_client.get_caller_identity()
        
        results['credentials_loaded'] = True
        results['account_id'] = identity['Account']
        results['user_arn'] = identity['Arn']
        
        print(f"‚úÖ AWS credentials valid for account: {identity['Account']}")
        
        # Check S3 access
        s3_client = boto3.client('s3')
        s3_client.list_buckets()
        results['s3_accessible'] = True
        print("‚úÖ S3 access verified")
        
        # Check specific bucket
        bucket_name = os.environ.get('AWS_S3_BUCKET')
        if bucket_name:
            try:
                s3_client.head_bucket(Bucket=bucket_name)
                results['bucket_exists'] = True
                results['bucket_name'] = bucket_name
                print(f"‚úÖ S3 bucket '{bucket_name}' exists and accessible")
            except ClientError as e:
                print(f"‚ö†Ô∏è S3 bucket '{bucket_name}' not accessible: {e}")
        
        # Get region
        region = os.environ.get('AWS_REGION', 'us-east-1')
        results['region'] = region
        print(f"‚úÖ Using AWS region: {region}")
        
    except NoCredentialsError:
        print("‚ùå AWS credentials not found")
    except Exception as e:
        print(f"‚ùå AWS verification failed: {e}")
    
    return results

def quick_colab_setup_streamlined(
    use_s3: bool = True,
    mount_drive: bool = True,
    run_tests: bool = False,  # Skip tests by default for speed
    force_reprocess: bool = False,
    debug_mode: bool = False,
    debug_family: str = 'FlatVel_A'
) -> Dict[str, Any]:
    """
    Streamlined Colab setup that skips unnecessary environment setup.
    Assumes setup_s3fs.py was run separately.
    
    Args:
        use_s3: Whether to use S3 for data operations
        mount_drive: Whether to mount Google Drive
        run_tests: Whether to run tests after setup
        force_reprocess: Whether to force reprocessing even if data exists
        debug_mode: Whether to enable debug mode (process only one family)
        debug_family: Which family to process in debug mode (default: 'FlatVel_A')
        
    Returns:
        Dict containing setup results
    """
    print("‚ö° Streamlined Colab Setup - Fast Mode")
    if debug_mode:
        print(f"üêõ DEBUG MODE ENABLED - Processing only family: {debug_family}")
    print("="*60)
    
    results = {}
    
    # Skip environment setup (assumes packages are already installed)
    print("‚è≠Ô∏è Skipping environment setup (assumes setup_s3fs.py was run)")
    results['environment'] = {
        'repo_cloned': True,
        'requirements_installed': True,
        'cuda_available': False,  # Will be detected during processing
        'gpu_count': 0,
        'environment': 'colab',
        'zarr_working': True,
        's3fs_working': True
    }
    
    # Skip CUDA setup check
    print("‚è≠Ô∏è Skipping CUDA setup check")
    results['cuda_working'] = False  # Will be detected during processing
    
    # Skip AWS setup (assumes credentials are already set)
    if use_s3:
        print("‚è≠Ô∏è Skipping AWS setup (assumes credentials are already set)")
        results['aws_credentials'] = 'assumed'
        results['aws_verification'] = {'s3_accessible': True, 'bucket_exists': True}
    else:
        results['aws_credentials'] = 'not_requested'
    
    # Skip Google Drive mounting if not requested
    if mount_drive:
        print("‚è≠Ô∏è Skipping Google Drive setup")
        results['drive_mounted'] = True  # Assume it's working
    else:
        results['drive_mounted'] = False
    
    # Skip dataset download
    results['dataset_download'] = False
    
    # Add src to Python path
    import sys
    sys.path.append('/content/YaleGWI/src')
    
    from src.core.config import CFG
    # Determine the correct input root based on whether we're using S3
    effective_input_root = CFG.s3_paths.raw_prefix if use_s3 else '/content/YaleGWI/train_samples'
    print(f"Effective input root: {effective_input_root}")

    # Go straight to preprocessing
    print("\n" + "="*50)
    print("STEP 5 & 6: Data Preprocessing")
    print("="*50)
    import logging
    logger = logging.getLogger()
    logger.info("Starting data preprocessing...")
    results['preprocessing'] = run_preprocessing(
        input_root=effective_input_root,
        output_root='/content/YaleGWI/preprocessed',
        use_s3=use_s3,
        save_to_drive=mount_drive,
        force_reprocess=force_reprocess,
        debug_mode=debug_mode,
        debug_family=debug_family
    )
    logger.info("Data preprocessing step finished.")

    # Skip training configuration
    print("‚è≠Ô∏è Skipping training configuration")
    results['training_config'] = {}
    
    # Skip tests unless specifically requested
    if run_tests:
        print("\n" + "="*50)
        print("STEP 8: Testing and Validation")
        print("="*50)
        results['tests'] = run_tests_and_validation()
    else:
        print("‚è≠Ô∏è Skipping tests for faster execution")
    
    print("\n" + "="*50)
    print("üéâ Streamlined Setup Complete!")
    print("="*50)
    
    # Minimal summary
    print("\nüìã Setup Summary:")
    preproc_result = results.get('preprocessing', {})
    preproc_success = preproc_result.get('success', False)
    print(f"  Preprocessing: {'‚úÖ' if preproc_success else '‚ùå'}")
    
    if debug_mode:
        print(f"  Debug Mode: ‚úÖ (processed only {debug_family})")
    
    return results

def check_and_fix_cuda_setup() -> bool:
    """
    Check CUDA setup and provide guidance for fixing issues.
    
    Returns:
        bool: True if CUDA is working properly
    """
    print("üîç Checking CUDA setup...")
    
    try:
        import torch
        import os
        
        # Check if we're in a GPU runtime
        if 'COLAB_GPU' not in os.environ:
            print("‚ùå Not in GPU runtime")
            print("üí° To enable GPU:")
            print("   1. Go to Runtime -> Change runtime type")
            print("   2. Select 'GPU' as Hardware accelerator")
            print("   3. Click 'Save' and restart the runtime")
            return False
        
        # Check CUDA availability
        if not torch.cuda.is_available():
            print("‚ùå CUDA not available despite GPU runtime")
            print("üí° This might be a temporary issue. Try:")
            print("   1. Restart the runtime (Runtime -> Restart runtime)")
            print("   2. Re-run the setup")
            return False
        
        # Test CUDA functionality
        try:
            test_tensor = torch.randn(100, 100).cuda()
            result = test_tensor.sum()
            print("‚úÖ CUDA is working properly")
            return True
        except Exception as e:
            print(f"‚ùå CUDA functionality test failed: {e}")
            print("üí° Try restarting the runtime")
            return False
            
    except ImportError:
        print("‚ùå PyTorch not installed")
        return False

def check_and_fix_zarr_installation() -> bool:
    """
    Check zarr installation and fix common issues.
    
    Returns:
        bool: True if zarr is working properly
    """
    print("üîç Checking zarr installation...")
    
    try:
        import zarr
        print(f"‚úÖ Zarr version: {zarr.__version__}")
        
        # Test basic zarr functionality
        try:
            # Test creating a simple array
            test_array = zarr.create((10, 10), dtype='float32')
            test_array[:] = 1.0
            print("‚úÖ Basic zarr functionality working")
            
            # Test compression with zarr 3.0.8 compatibility
            try:
                # Skip compression entirely - just test basic functionality
                test_compressed = zarr.create((10, 10), dtype='float32')
                test_compressed[:] = 1.0
                print("‚úÖ Zarr functionality working (no compression)")
                return True
            except Exception as comp_error:
                print(f"‚ùå Zarr functionality failed: {comp_error}")
                return False
                
        except Exception as e:
            print(f"‚ùå Basic zarr functionality failed: {e}")
            return False
            
    except ImportError:
        print("‚ùå Zarr not installed")
        print("üí° Installing zarr...")
        try:
            subprocess.run([sys.executable, '-m', 'pip', 'install', 'zarr'], check=True)
            print("‚úÖ Zarr installed successfully")
            return True
        except subprocess.CalledProcessError as e:
            print(f"‚ùå Failed to install zarr: {e}")
            return False

def check_and_fix_s3fs_installation() -> bool:
    """
    Check s3fs installation and update if needed to fix compatibility issues.
    
    Returns:
        bool: True if s3fs is working properly
    """
    print("üîç Checking s3fs installation...")
    
    try:
        import s3fs
        print(f"‚úÖ S3fs version: {s3fs.__version__}")
        
        # Check if version is too old (causing compatibility issues)
        # Old versions like 0.4.2 cause performance problems and 'asynchronous' parameter issues
        version_parts = s3fs.__version__.split('.')
        
        # Handle both old format (0.x.x) and new format (2023.1.0)
        if len(version_parts) >= 2:
            if version_parts[0].isdigit() and version_parts[1].isdigit():
                major = int(version_parts[0])
                minor = int(version_parts[1])
                
                # Check for old format (0.x.x) or new format but too old
                if major == 0 or (major < 2024):
                    print("‚ö†Ô∏è S3fs version is old and may cause compatibility issues")
                    return _update_s3fs()
                else:
                    print("‚úÖ S3fs version is recent and should work properly")
                    # Test functionality
                    try:
                        fs = s3fs.S3FileSystem(anon=True)
                        print("‚úÖ S3fs functionality verified")
                        return True
                    except Exception as e:
                        if "asynchronous" in str(e):
                            print(f"‚ùå S3fs has 'asynchronous' issue despite recent version: {e}")
                            return _update_s3fs()
                        else:
                            print(f"‚úÖ S3fs working (different error: {e})")
                            return True
            else:
                print("‚ö†Ô∏è Could not parse s3fs version, assuming it needs update")
                return _update_s3fs()
        else:
            print("‚ö†Ô∏è Could not parse s3fs version, assuming it needs update")
            return _update_s3fs()
            
    except ImportError:
        print("‚ùå S3fs not installed")
        print("üí° Installing s3fs...")
        try:
            subprocess.run([sys.executable, '-m', 'pip', 'install', 's3fs>=2024.1.0'], check=True)
            print("‚úÖ S3fs installed successfully")
            return True
        except subprocess.CalledProcessError as e:
            print(f"‚ùå Failed to install s3fs: {e}")
            return False

def _update_s3fs() -> bool:
    """
    Helper function to update s3fs to a working version.
    
    Returns:
        bool: True if update was successful
    """
    try:
        print("üí° Updating s3fs to latest version...")
        
        # Force reinstall s3fs to latest version
        print("  Uninstalling old s3fs...")
        subprocess.run([sys.executable, '-m', 'pip', 'uninstall', '-y', 's3fs'], check=True)
        
        print("  Installing latest s3fs...")
        # Try to install a much newer version that fixes the 'asynchronous' issue
        subprocess.run([sys.executable, '-m', 'pip', 'install', 's3fs>=2023.12.0', '--no-cache-dir', '--force-reinstall'], check=True)
        
        print("‚úÖ S3fs updated successfully")
        
        # Force reload s3fs to get new version
        import importlib
        import s3fs
        importlib.reload(s3fs)
        print(f"‚úÖ New s3fs version: {s3fs.__version__}")
        
        # Test that the fix worked
        try:
            fs = s3fs.S3FileSystem(anon=True)
            print("‚úÖ S3fs functionality verified after update")
            return True
        except Exception as e:
            if "asynchronous" in str(e):
                print(f"‚ùå S3fs still has 'asynchronous' issue after update: {e}")
                # Try alternative approach with even newer version
                print("üí° Trying alternative s3fs installation...")
                try:
                    subprocess.run([sys.executable, '-m', 'pip', 'install', 's3fs>=2024.1.0', '--no-cache-dir', '--force-reinstall'], check=True)
                    importlib.reload(s3fs)
                    fs = s3fs.S3FileSystem(anon=True)
                    print("‚úÖ S3fs fixed with alternative installation")
                    return True
                except Exception as e2:
                    print(f"‚ùå Alternative installation also failed: {e2}")
                    # Try final approach - completely clean install
                    print("üí° Trying final s3fs fix...")
                    try:
                        subprocess.run([sys.executable, '-m', 'pip', 'uninstall', '-y', 's3fs', 'fsspec'], check=True)
                        subprocess.run([sys.executable, '-m', 'pip', 'install', 'fsspec>=2024.1.0'], check=True)
                        subprocess.run([sys.executable, '-m', 'pip', 'install', 's3fs>=2024.1.0'], check=True)
                        importlib.reload(s3fs)
                        fs = s3fs.S3FileSystem(anon=True)
                        print("‚úÖ S3fs fixed with final approach")
                        return True
                    except Exception as e3:
                        print(f"‚ùå All s3fs fixes failed: {e3}")
                        print("‚ö†Ô∏è S3 operations will be disabled - using local processing only")
                        return False
            else:
                print(f"‚úÖ S3fs working (different error: {e})")
                return True
                
    except subprocess.CalledProcessError as e:
        print(f"‚ùå Failed to update s3fs: {e}")
        print("‚ö†Ô∏è Continuing with old version - S3 operations may fail")
        return False

if __name__ == "__main__":
    # Example usage
    results = complete_colab_setup(use_s3=True)
    print("\nSetup completed with results:", results) 