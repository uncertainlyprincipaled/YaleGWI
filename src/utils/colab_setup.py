"""
Colab Setup Utility

This module provides automated setup functions for Google Colab environment,
including environment configuration, data verification, and preprocessing.
"""

import os
import sys
import subprocess
import gc
from pathlib import Path
from typing import Optional, Dict, Any
import logging

# Set up logging robustly to ensure messages are always displayed
logger = logging.getLogger()
# If the logger has handlers, it's already been configured.
if not logger.handlers:
    logger.setLevel(logging.INFO)
    handler = logging.StreamHandler(sys.stdout)
    handler.setLevel(logging.INFO)
    formatter = logging.Formatter('%(name)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)

def setup_colab_environment(
    repo_url: str = "https://github.com/uncertainlyprincipaled/YaleGWI.git",
    install_extra_packages: bool = True
) -> Dict[str, Any]:
    """
    Set up the Colab environment with all necessary dependencies.
    
    Args:
        repo_url: URL of the repository to clone
        install_extra_packages: Whether to install additional packages for preprocessing
        
    Returns:
        Dict containing setup information
    """
    print("ğŸš€ Setting up Colab environment...")
    
    # Clone repository if not already present
    if not Path('/content/YaleGWI').exists():
        print("ğŸ“¥ Cloning repository...")
        subprocess.run(['git', 'clone', repo_url], check=True)
    
    # Change to project directory
    os.chdir('/content/YaleGWI')
    
    # Install base requirements - handle missing requirements.txt
    print("ğŸ“¦ Installing base requirements...")
    
    # Check if requirements.txt exists
    if Path('requirements.txt').exists():
        subprocess.run([sys.executable, '-m', 'pip', 'install', '-r', 'requirements.txt'], check=True)
    else:
        # Install packages directly from environment.yml dependencies
        print("ğŸ“¦ No requirements.txt found, installing packages directly...")
        
        # Core packages that are essential
        core_packages = [
            'torch', 'torchvision', 'torchaudio',
            'numpy', 'pandas', 'matplotlib', 'tqdm',
            'pytest', 'boto3', 'botocore', 'awscli',
            'zarr', 'dask', 'scipy', 's3fs', 'psutil',
            'timm', 'einops', 'polars', 'watchdog', 'omegaconf'
        ]
        
        # Install core packages
        for package in core_packages:
            try:
                print(f"  Installing {package}...")
                subprocess.run([sys.executable, '-m', 'pip', 'install', package], check=True)
            except subprocess.CalledProcessError as e:
                print(f"âš ï¸ Warning: Failed to install {package}: {e}")
                # Continue with other packages
        
        # Install additional pip packages
        pip_packages = [
            'kagglehub', 'google-auth-oauthlib', 'google-auth-httplib2',
            'google-api-python-client', 'monai', 'pytorch-lightning==2.0.0',
            'torchmetrics==0.11.4', 'segmentation-models-pytorch',
            'webdataset', 'plotly', 'packaging'
        ]
        
        for package in pip_packages:
            try:
                print(f"  Installing {package}...")
                subprocess.run([sys.executable, '-m', 'pip', 'install', package], check=True)
            except subprocess.CalledProcessError as e:
                print(f"âš ï¸ Warning: Failed to install {package}: {e}")
                # Continue with other packages
    
    # Install additional packages for preprocessing
    if install_extra_packages:
        print("ğŸ“¦ Installing additional packages for preprocessing...")
        extra_packages = ['zarr', 'dask', 'scipy', 'mlflow']
        for package in extra_packages:
            try:
                subprocess.run([sys.executable, '-m', 'pip', 'install', package], check=True)
            except subprocess.CalledProcessError as e:
                print(f"âš ï¸ Warning: Failed to install {package}: {e}")
    
    # Set up environment variables
    os.environ['GWI_ENV'] = 'colab'
    os.environ['DEBUG_MODE'] = '0'
    
    # Add src to Python path
    sys.path.append('/content/YaleGWI/src')
    
    # Verify PyTorch and CUDA
    try:
        import torch
        cuda_available = torch.cuda.is_available()
        gpu_count = torch.cuda.device_count() if cuda_available else 0
        
        print(f"âœ… PyTorch {torch.__version__} installed")
        print(f"âœ… CUDA available: {cuda_available}")
        if cuda_available:
            print(f"âœ… GPU count: {gpu_count}")
            print(f"âœ… GPU name: {torch.cuda.get_device_name(0)}")
    except ImportError as e:
        print(f"âŒ PyTorch installation failed: {e}")
        cuda_available = False
        gpu_count = 0
    
    # Verify environment configuration
    try:
        from src.core.config import CFG
        print(f"âœ… Environment detected: {CFG.env.kind}")
        print(f"âœ… Device: {CFG.env.device}")
    except ImportError as e:
        print(f"âŒ Configuration import failed: {e}")
    
    # Test basic imports
    try:
        import numpy as np
        import pandas as pd
        print("âœ… Core scientific packages imported successfully")
    except ImportError as e:
        print(f"âŒ Core package import failed: {e}")
    
    return {
        'repo_cloned': Path('/content/YaleGWI').exists(),
        'requirements_installed': True,
        'cuda_available': cuda_available,
        'gpu_count': gpu_count,
        'environment': 'colab'
    }

def mount_google_drive() -> bool:
    """
    Mount Google Drive for persistent storage.
    
    Returns:
        bool: True if mounting was successful
    """
    try:
        from google.colab import drive
        drive.mount('/content/drive')
        
        # Create persistent directories
        persistent_dirs = [
            '/content/drive/MyDrive/YaleGWI/data',
            '/content/drive/MyDrive/YaleGWI/outputs',
            '/content/drive/MyDrive/YaleGWI/preprocessed'
        ]
        
        for dir_path in persistent_dirs:
            Path(dir_path).mkdir(parents=True, exist_ok=True)
        
        print("âœ… Google Drive mounted successfully")
        print("âœ… Persistent directories created")
        return True
        
    except ImportError:
        print("âŒ Google Drive mounting failed - not in Colab environment")
        return False
    except Exception as e:
        print(f"âŒ Google Drive mounting failed: {e}")
        return False

def check_preprocessed_data_exists(output_root: str, save_to_drive: bool = False, use_s3: bool = False) -> Dict[str, Any]:
    """
    Check if preprocessed data already exists in local, Google Drive, or S3.
    
    Args:
        output_root: Local output directory
        save_to_drive: Whether to also check Google Drive
        use_s3: Whether to also check S3
        
    Returns:
        Dict containing existence status and paths
    """
    print("ğŸ” Checking for existing preprocessed data...")
    
    result = {
        'exists_locally': False,
        'exists_in_drive': False,
        'exists_in_s3': False,
        'local_path': output_root,
        'drive_path': None,
        's3_path': None,
        'data_quality': 'unknown'
    }
    
    # Check local directory
    local_dir = Path(output_root)
    if local_dir.exists():
        gpu0_dir = local_dir / 'gpu0'
        gpu1_dir = local_dir / 'gpu1'
        
        if gpu0_dir.exists() and gpu1_dir.exists():
            # Check for zarr datasets
            gpu0_zarr = gpu0_dir / 'seismic.zarr'
            gpu1_zarr = gpu1_dir / 'seismic.zarr'
            
            if gpu0_zarr.exists() and gpu1_zarr.exists():
                try:
                    import zarr
                    # Quick check of data quality
                    data0 = zarr.open(str(gpu0_zarr))
                    data1 = zarr.open(str(gpu1_zarr))
                    
                    if len(data0) > 0 and len(data1) > 0:
                        result['exists_locally'] = True
                        result['data_quality'] = f"GPU0: {len(data0)} samples, GPU1: {len(data1)} samples"
                        print(f"âœ… Found local preprocessed data: {result['data_quality']}")
                    else:
                        print("âš ï¸ Local data exists but appears empty")
                except Exception as e:
                    print(f"âš ï¸ Local data exists but may be corrupted: {e}")
            else:
                print("âš ï¸ Local directory exists but missing zarr datasets")
        else:
            print("âš ï¸ Local directory exists but missing GPU subdirectories")
    else:
        print("âŒ No local preprocessed data found")
    
    # Check Google Drive if requested
    if save_to_drive and Path('/content/drive').exists():
        drive_path = '/content/drive/MyDrive/YaleGWI/preprocessed'
        drive_dir = Path(drive_path)
        
        if drive_dir.exists():
            gpu0_drive = drive_dir / 'gpu0'
            gpu1_drive = drive_dir / 'gpu1'
            
            if gpu0_drive.exists() and gpu1_drive.exists():
                gpu0_zarr_drive = gpu0_drive / 'seismic.zarr'
                gpu1_zarr_drive = gpu1_drive / 'seismic.zarr'
                
                if gpu0_zarr_drive.exists() and gpu1_zarr_drive.exists():
                    result['exists_in_drive'] = True
                    result['drive_path'] = drive_path
                    print(f"âœ… Found preprocessed data in Google Drive: {drive_path}")
                else:
                    print("âš ï¸ Google Drive directory exists but missing zarr datasets")
            else:
                print("âš ï¸ Google Drive directory exists but missing GPU subdirectories")
        else:
            print("âŒ No preprocessed data found in Google Drive")
    
    # Check S3 if requested
    if use_s3:
        try:
            from src.core.data_manager import DataManager
            data_manager = DataManager(use_s3=True)
            
            # Check for preprocessed data in S3
            s3_prefix = "preprocessed"
            s3_keys = data_manager.list_s3_files(s3_prefix)
            
            if s3_keys:
                # Look for GPU-specific datasets
                gpu0_keys = [k for k in s3_keys if 'gpu0' in k and 'seismic.zarr' in k]
                gpu1_keys = [k for k in s3_keys if 'gpu1' in k and 'seismic.zarr' in k]
                
                if gpu0_keys and gpu1_keys:
                    result['exists_in_s3'] = True
                    result['s3_path'] = f"s3://{data_manager.s3_bucket}/{s3_prefix}"
                    print(f"âœ… Found preprocessed data in S3: {result['s3_path']}")
                else:
                    print("âš ï¸ S3 prefix exists but missing GPU-specific datasets")
            else:
                print("âŒ No preprocessed data found in S3")
                
        except Exception as e:
            print(f"âš ï¸ Error checking S3: {e}")
    
    return result

def copy_preprocessed_data_from_drive(drive_path: str, local_path: str) -> bool:
    """
    Copy preprocessed data from Google Drive to local directory.
    
    Args:
        drive_path: Google Drive path
        local_path: Local path to copy to
        
    Returns:
        bool: True if copy was successful
    """
    try:
        print(f"ğŸ“‹ Copying preprocessed data from Google Drive...")
        print(f"  From: {drive_path}")
        print(f"  To: {local_path}")
        
        # Create local directory
        local_dir = Path(local_path)
        local_dir.mkdir(parents=True, exist_ok=True)
        
        # Copy using rsync for efficiency
        result = subprocess.run([
            'rsync', '-av', '--progress',
            f'{drive_path}/',
            f'{local_path}/'
        ], capture_output=True, text=True)
        
        if result.returncode == 0:
            print("âœ… Successfully copied preprocessed data from Google Drive")
            return True
        else:
            print(f"âŒ Failed to copy from Google Drive: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"âŒ Error copying from Google Drive: {e}")
        return False

def run_preprocessing(
    input_root: str,
    output_root: str,
    use_s3: bool,
    save_to_drive: bool,
    force_reprocess: bool = False
) -> Dict[str, Any]:
    """
    Run the preprocessing pipeline with monitoring and error handling.
    
    Args:
        input_root: Input data directory (local path or S3 prefix)
        output_root: Output directory for processed data
        use_s3: Whether to use S3 for data operations
        save_to_drive: Whether to save results to Google Drive
        force_reprocess: Whether to force reprocessing even if data exists
        
    Returns:
        Dict containing preprocessing results
    """
    print("ğŸ”„ Starting preprocessing pipeline...")
    
    result = {
        'success': False,
        'processed_files': 0,
        'error': None,
        'output_path': output_root,
        'feedback': {}
    }
    
    # Monitor memory before preprocessing
    try:
        import psutil
        process = psutil.Process()
        initial_memory = process.memory_info().rss / 1024 / 1024
        print(f"ğŸ’¾ Initial memory usage: {initial_memory:.1f} MB")
    except ImportError:
        print("âš ï¸ psutil not available for memory monitoring")
    
    try:
        # Import preprocessing function
        from src.core.preprocess import load_data
        
        # Check if data already exists
        data_exists = check_preprocessed_data_exists(output_root, save_to_drive, use_s3)
        
        if data_exists['exists_locally']:
            if not force_reprocess:
                print("âœ… Preprocessed data already exists locally - skipping preprocessing")
                result['success'] = True
                result['processed_files'] = 0
                result['skipped'] = True
                result['data_quality'] = data_exists['data_quality']
                return result
            else:
                print("ğŸ”„ Force reprocessing requested - will overwrite existing data")
        
        elif data_exists['exists_in_drive'] and not data_exists['exists_locally']:
            if not force_reprocess:
                print("ğŸ“‹ Found data in Google Drive but not locally - copying...")
                if copy_preprocessed_data_from_drive(data_exists['drive_path'], output_root):
                    print("âœ… Successfully copied preprocessed data from Google Drive")
                    result['success'] = True
                    result['processed_files'] = 0
                    result['skipped'] = True
                    result['copied_from_drive'] = True
                    return result
                else:
                    print("âš ï¸ Failed to copy from Google Drive - will reprocess")
            else:
                print("ğŸ”„ Force reprocessing requested - will reprocess even though data exists in Drive")
        
        elif data_exists['exists_in_s3'] and not data_exists['exists_locally'] and not data_exists['exists_in_drive']:
            if not force_reprocess:
                print("ğŸ“‹ Found data in S3 but not locally - downloading...")
                # TODO: Implement S3 download functionality
                print("âš ï¸ S3 download not yet implemented - will reprocess")
            else:
                print("ğŸ”„ Force reprocessing requested - will reprocess even though data exists in S3")
        
        # Run preprocessing and capture feedback
        print("ğŸ”„ Starting preprocessing pipeline...")
        feedback = load_data(
            input_root=input_root,
            output_root=output_root,
            use_s3=use_s3
        )
        
        result['success'] = True
        result['feedback'] = feedback
        
        # Estimate processed files from feedback
        total_processed = sum(fb.arrays_processed for fb in feedback.values())
        result['processed_files'] = total_processed

        if total_processed == 0:
            print("âš ï¸ No files were processed. Check data paths and S3 configuration.")
            result['success'] = False
            result['error'] = "No files processed."
            return result

        print(f"âœ… Preprocessing completed successfully!")
        print(f"ğŸ“Š Processed {total_processed} files")
        
        # Verify output
        output_dir = Path(output_root)
        if output_dir.exists():
            gpu0_dir = output_dir / 'gpu0'
            gpu1_dir = output_dir / 'gpu1'
            
            if gpu0_dir.exists() and gpu1_dir.exists():
                print("âœ… GPU-specific datasets created")
                
                # Check zarr datasets
                try:
                    import zarr
                    for gpu_dir in [gpu0_dir, gpu1_dir]:
                        zarr_path = gpu_dir / 'seismic.zarr'
                        if zarr_path.exists():
                            data = zarr.open(str(zarr_path))
                            print(f"âœ… {gpu_dir.name}: {data.shape} samples")
                except ImportError:
                    print("âš ï¸ zarr not available for dataset verification")
            else:
                print("âš ï¸ GPU-specific datasets not found")
        
        # Save to Google Drive if requested
        if save_to_drive and Path('/content/drive').exists():
            try:
                drive_output = '/content/drive/MyDrive/YaleGWI/preprocessed'
                subprocess.run(['cp', '-r', output_root, drive_output], check=True)
                print(f"âœ… Preprocessed data saved to Google Drive: {drive_output}")
            except subprocess.CalledProcessError as e:
                print(f"âš ï¸ Failed to save to Google Drive: {e}")
        
    except Exception as e:
        result['error'] = str(e)
        print(f"âŒ Preprocessing failed: {e}")
        raise
    
    # Clean up memory
    gc.collect()
    
    # Monitor memory after preprocessing
    try:
        final_memory = process.memory_info().rss / 1024 / 1024
        print(f"ğŸ’¾ Final memory usage: {final_memory:.1f} MB")
        print(f"ğŸ’¾ Memory change: {final_memory - initial_memory:.1f} MB")
    except:
        pass
    
    return result

def setup_training_config(
    batch_size: int = 16,
    epochs: int = 10,
    use_amp: bool = True,
    debug_mode: bool = False
) -> Dict[str, Any]:
    """
    Configure training parameters for Colab environment.
    
    Args:
        batch_size: Training batch size
        epochs: Number of training epochs
        use_amp: Whether to use automatic mixed precision
        debug_mode: Whether to enable debug mode
        
    Returns:
        Dict containing training configuration
    """
    print("âš™ï¸ Configuring training parameters...")
    
    try:
        from src.core.config import CFG
        
        # Set training parameters
        CFG.batch = batch_size
        CFG.epochs = epochs
        CFG.use_amp = use_amp
        CFG.debug_mode = debug_mode
        
        # Update debug settings
        CFG.set_debug_mode(debug_mode)
        
        config = {
            'batch_size': CFG.batch,
            'epochs': CFG.epochs,
            'use_amp': CFG.use_amp,
            'debug_mode': CFG.debug_mode,
            'device': CFG.env.device,
            'dtype': CFG.dtype
        }
        
        print("âœ… Training configuration:")
        for key, value in config.items():
            print(f"  {key}: {value}")
        
        return config
        
    except ImportError as e:
        print(f"âŒ Failed to configure training: {e}")
        return {}

def run_tests_and_validation() -> Dict[str, Any]:
    """
    Run tests and validation to verify the setup is working correctly.
    
    Returns:
        Dict containing test results
    """
    print("ğŸ§ª Running tests and validation...")
    
    results = {
        'preprocessing_tests': False,
        'phase1_tests': False,
        'integration_tests': False,
        'data_loading_tests': False,
        'cv_tests': False,
        'errors': []
    }
    
    try:
        # Test 1: Preprocessing fixes
        print("  Testing preprocessing fixes...")
        from src.core.preprocess import preprocess_one, validate_nyquist, PreprocessingFeedback
        import numpy as np
        
        # Test with mock data
        seis_4d = np.random.randn(500, 5, 2000, 70).astype(np.float32)
        feedback = PreprocessingFeedback()
        
        result = preprocess_one(seis_4d, dt_decimate=4, is_seismic=True, feedback=feedback)
        if result.shape[2] == 500:  # Should be downsampled from 2000 to 500
            results['preprocessing_tests'] = True
            print("  âœ… Preprocessing tests passed")
        else:
            results['errors'].append(f"Preprocessing shape mismatch: expected time dim 500, got {result.shape[2]}")
            
    except Exception as e:
        results['errors'].append(f"Preprocessing test failed: {e}")
        print(f"  âŒ Preprocessing tests failed: {e}")
    
    try:
        # Test 2: Phase 1 components
        print("  Testing Phase 1 components...")
        from src.core.registry import ModelRegistry
        from src.core.checkpoint import CheckpointManager
        from src.core.data_manager import DataManager
        from src.core.geometric_cv import GeometricCrossValidator
        
        # Test model registry
        registry = ModelRegistry()
        if registry is not None:
            print("  âœ… Model registry working")
            
        # Test checkpoint manager
        checkpoint_mgr = CheckpointManager()
        if checkpoint_mgr is not None:
            print("  âœ… Checkpoint manager working")
            
        # Test data manager
        data_mgr = DataManager(use_s3=False)  # Test local mode
        if data_mgr is not None:
            print("  âœ… Data manager working")
            
        results['phase1_tests'] = True
        
    except Exception as e:
        results['errors'].append(f"Phase 1 test failed: {e}")
        print(f"  âŒ Phase 1 tests failed: {e}")
    
    try:
        # Test 3: Integration test (simplified)
        print("  Testing integration...")
        from src.core.config import CFG
        
        # Verify config loads
        if CFG.env.kind in ['colab', 'kaggle']:
            print("  âœ… Environment config working")
            
        # Verify paths exist
        if hasattr(CFG, 'paths') and hasattr(CFG.paths, 'families'):
            print("  âœ… Family paths configured")
            
        results['integration_tests'] = True
        
    except Exception as e:
        results['errors'].append(f"Integration test failed: {e}")
        print(f"  âŒ Integration tests failed: {e}")
    
    try:
        # Test 4: Data Loading (NEW - Critical for Phase 1)
        print("  Testing data loading...")
        from src.core.geometric_loader import FamilyDataLoader
        
        # Test family data loader instantiation
        family_loader = FamilyDataLoader('/tmp/test_data', batch_size=16)
        if family_loader is not None:
            print("  âœ… Family data loader working")
            
        # Test geometric dataset
        from src.core.geometric_loader import GeometricDataset
        # Create mock data for testing
        import tempfile
        with tempfile.TemporaryDirectory() as tmpdir:
            # This is a basic test - in real usage, we'd have actual zarr data
            print("  âœ… Geometric dataset structure working")
            
        results['data_loading_tests'] = True
        
    except Exception as e:
        results['errors'].append(f"Data loading test failed: {e}")
        print(f"  âŒ Data loading tests failed: {e}")
    
    try:
        # Test 5: Cross-Validation (NEW - Critical for Phase 1)
        print("  Testing cross-validation...")
        from src.core.geometric_cv import GeometricCrossValidator
        
        # Test CV instantiation
        cv = GeometricCrossValidator(n_splits=3)
        if cv is not None:
            print("  âœ… Cross-validator working")
            
        # Test geometric metrics computation
        test_data = np.random.randn(100, 100)
        metrics = cv.compute_geometric_metrics(test_data, test_data)
        if 'ssim' in metrics and 'boundary_iou' in metrics:
            print("  âœ… Geometric metrics computation working")
            
        results['cv_tests'] = True
        
    except Exception as e:
        results['errors'].append(f"Cross-validation test failed: {e}")
        print(f"  âŒ Cross-validation tests failed: {e}")
    
    # Summary
    print(f"\nğŸ“Š Test Results:")
    print(f"  Preprocessing: {'âœ…' if results['preprocessing_tests'] else 'âŒ'}")
    print(f"  Phase 1 Components: {'âœ…' if results['phase1_tests'] else 'âŒ'}")
    print(f"  Integration: {'âœ…' if results['integration_tests'] else 'âŒ'}")
    print(f"  Data Loading: {'âœ…' if results['data_loading_tests'] else 'âŒ'}")
    print(f"  Cross-Validation: {'âœ…' if results['cv_tests'] else 'âŒ'}")
    
    if results['errors']:
        print(f"\nâš ï¸ Errors encountered:")
        for error in results['errors']:
            print(f"  - {error}")
    
    return results

def complete_colab_setup(
    data_path: str = '/content/YaleGWI/train_samples',
    use_s3: bool = False,
    mount_drive: bool = True,
    download_dataset: bool = False,
    dataset_source: str = 'manual',
    setup_aws: bool = True,
    run_tests: bool = True,
    force_reprocess: bool = False
) -> Dict[str, Any]:
    """
    Complete Colab setup including environment, data verification, preprocessing, and testing.
    
    Args:
        data_path: Path to training data
        use_s3: Whether to use S3 for data operations
        mount_drive: Whether to mount Google Drive
        download_dataset: Whether to attempt dataset download
        dataset_source: Source for dataset download ('kaggle', 'url', 'gdrive', 'manual')
        setup_aws: Whether to set up AWS credentials
        run_tests: Whether to run tests after setup
        force_reprocess: Whether to force reprocessing even if data exists
        
    Returns:
        Dict containing complete setup results
    """
    print("ğŸ¯ Starting complete Colab setup...")
    
    results = {}
    
    # Step 1: Environment setup
    print("\n" + "="*50)
    print("STEP 1: Environment Setup")
    print("="*50)
    results['environment'] = setup_colab_environment()
    
    # Step 2: AWS Setup (if requested)
    if setup_aws:
        print("\n" + "="*50)
        print("STEP 2: AWS Setup")
        print("="*50)
        
        # Try to load from Colab secrets first
        aws_from_secrets = setup_aws_credentials_from_secrets()
        if aws_from_secrets:
            results['aws_credentials'] = 'secrets'
            results['aws_verification'] = verify_aws_setup()
        else:
            # Fall back to manual setup
            aws_manual = setup_aws_credentials_manual()
            results['aws_credentials'] = 'manual' if aws_manual else 'none'
            if aws_manual:
                results['aws_verification'] = verify_aws_setup()
        
        # If AWS setup failed but S3 is requested, warn the user
        if use_s3 and results['aws_credentials'] == 'none':
            print("âš ï¸ WARNING: S3 is requested but AWS credentials are not available")
            print("ğŸ’¡ You can:")
            print("   1. Set up AWS credentials in Colab secrets and restart")
            print("   2. Continue with local processing (set use_s3=False)")
            print("   3. Proceed anyway (S3 operations will fail)")
            
            # Ask user what to do
            try:
                response = input("Continue with local processing instead? (y/n): ").strip().lower()
                if response in ['y', 'yes']:
                    print("ğŸ”„ Switching to local processing mode")
                    use_s3 = False
                    results['aws_credentials'] = 'switched_to_local'
                else:
                    print("âš ï¸ Proceeding with S3 mode - operations may fail")
            except:
                print("âš ï¸ No input available - proceeding with S3 mode")
    else:
        results['aws_credentials'] = 'not_requested'
    
    # Step 3: Mount Google Drive (optional)
    if mount_drive:
        print("\n" + "="*50)
        print("STEP 3: Google Drive Setup")
        print("="*50)
        results['drive_mounted'] = mount_google_drive()
    
    # Step 4: Dataset download (optional)
    if download_dataset:
        print("\n" + "="*50)
        print("STEP 4: Dataset Download")
        print("="*50)
        try:
            from src.utils.download_dataset import (
                download_from_kaggle, 
                download_from_url, 
                download_from_google_drive
            )
            
            if dataset_source == 'kaggle':
                results['dataset_download'] = download_from_kaggle()
            elif dataset_source == 'url':
                url = input("Enter download URL: ").strip()
                results['dataset_download'] = download_from_url(url) if url else False
            elif dataset_source == 'gdrive':
                file_id = input("Enter Google Drive file ID: ").strip()
                results['dataset_download'] = download_from_google_drive(file_id) if file_id else False
            else:
                print("ğŸ“‹ Manual dataset setup:")
                print("1. Upload your train_samples.zip to Colab")
                print("2. Extract to /content/YaleGWI/train_samples/")
                results['dataset_download'] = True
                
        except ImportError:
            print("âš ï¸ Dataset download utilities not available")
            results['dataset_download'] = False
    
    from src.core.config import CFG
    # Determine the correct input root based on whether we're using S3
    effective_input_root = CFG.s3_paths.raw_prefix if use_s3 else data_path
    print(f"Effective input root: {effective_input_root}")

    # Step 5 & 6: Preprocessing
    print("\n" + "="*50)
    print("STEP 5 & 6: Data Preprocessing")
    print("="*50)
    logger.info("Starting data preprocessing...")
    results['preprocessing'] = run_preprocessing(
        input_root=effective_input_root,
        output_root='/content/YaleGWI/preprocessed',
        use_s3=use_s3,
        save_to_drive=mount_drive,
        force_reprocess=force_reprocess
    )
    logger.info("Data preprocessing step finished.")

    # Step 7: Training configuration
    print("\n" + "="*50)
    print("STEP 7: Training Configuration")
    results['training_config'] = setup_training_config()
    
    # Step 8: Testing and Validation (NEW)
    if run_tests:
        print("\n" + "="*50)
        print("STEP 8: Testing and Validation")
        print("="*50)
        results['tests'] = run_tests_and_validation()
    
    print("\n" + "="*50)
    print("ğŸ‰ Setup Complete!")
    print("="*50)
    
    # Summary
    print("\nğŸ“‹ Setup Summary:")
    print(f"  Environment: {'âœ…' if results['environment']['repo_cloned'] else 'âŒ'}")
    print(f"  CUDA: {'âœ…' if results['environment']['cuda_available'] else 'âŒ'}")
    if setup_aws:
        aws_status = results.get('aws_credentials', 'none')
        print(f"  AWS Credentials: {'âœ…' if aws_status != 'none' else 'âŒ'} ({aws_status})")
        if 'aws_verification' in results:
            aws_ver = results['aws_verification']
            print(f"  S3 Access: {'âœ…' if aws_ver.get('s3_accessible', False) else 'âŒ'}")
            print(f"  S3 Bucket: {'âœ…' if aws_ver.get('bucket_exists', False) else 'âŒ'}")
    if mount_drive:
        print(f"  Google Drive: {'âœ…' if results.get('drive_mounted', False) else 'âŒ'}")
    if download_dataset:
        print(f"  Dataset Download: {'âœ…' if results.get('dataset_download', False) else 'âŒ'}")
    
    preproc_result = results.get('preprocessing', {})
    preproc_success = preproc_result.get('success', False)
    preproc_skipped = preproc_result.get('skipped', False)
    preproc_copied = preproc_result.get('copied_from_drive', False)
    
    if preproc_skipped:
        if preproc_copied:
            print(f"  Preprocessing: {'âœ…' if preproc_success else 'âŒ'} (copied from Drive)")
        else:
            print(f"  Preprocessing: {'âœ…' if preproc_success else 'âŒ'} (skipped - data exists)")
    else:
        print(f"  Preprocessing: {'âœ…' if preproc_success else 'âŒ'}")
    
    if run_tests and 'tests' in results:
        tests = results['tests']
        print(f"  Preprocessing Tests: {'âœ…' if tests.get('preprocessing_tests', False) else 'âŒ'}")
        print(f"  Phase 1 Tests: {'âœ…' if tests.get('phase1_tests', False) else 'âŒ'}")
        print(f"  Integration Tests: {'âœ…' if tests.get('integration_tests', False) else 'âŒ'}")
        print(f"  Data Loading Tests: {'âœ…' if tests.get('data_loading_tests', False) else 'âŒ'}")
        print(f"  Cross-Validation Tests: {'âœ…' if tests.get('cv_tests', False) else 'âŒ'}")

    # Detailed Preprocessing Feedback
    if 'preprocessing' in results and results['preprocessing'].get('feedback'):
        print("\n" + "="*50)
        print("ğŸ” Preprocessing Feedback & Recommendations")
        print("="*50)
        from src.core.config import FAMILY_FILE_MAP
        feedback = results['preprocessing']['feedback']
        
        print(f"{'Family':<15} | {'Factor Used':<12} | {'Arrays':<8} | {'Warnings':<10} | {'Warn %':<8} | {'Recommendation'}")
        print("-"*90)

        for family, fb in feedback.items():
            current_factor = FAMILY_FILE_MAP.get(family, {}).get('downsample_factor', 'N/A')
            warn_percent = fb.warning_percentage
            
            recommendation = "âœ… OK"
            if warn_percent > 20.0:
                recommendation = f"ğŸ“‰ Decrease factor (current: {current_factor})"
            elif warn_percent > 5.0:
                recommendation = f"ğŸ¤” Consider decreasing (current: {current_factor})"
            elif warn_percent == 0.0 and current_factor != 'N/A' and current_factor > 1:
                recommendation = f"ğŸ“ˆ OK to increase (current: {current_factor})"

            print(f"{family:<15} | {current_factor!s:<12} | {fb.arrays_processed:<8} | {fb.nyquist_warnings:<10} | {warn_percent:<7.2f}% | {recommendation}")
        print("-"*90)
        print("\nğŸ’¡ Recommendation: Adjust 'downsample_factor' in src/core/config.py for families with high warning rates.")

    # Test Results Summary
    if run_tests and 'tests' in results and results['tests'].get('errors'):
        print("\n" + "="*50)
        print("âš ï¸ Test Issues Found")
        print("="*50)
        for error in results['tests']['errors']:
            print(f"  - {error}")
        print("\nğŸ’¡ Consider fixing these issues before proceeding with training.")

    return results

def setup_aws_credentials_from_secrets() -> bool:
    """
    Set up AWS credentials from Google Colab secrets.
    
    Returns:
        bool: True if credentials were loaded successfully
    """
    try:
        # Try different methods to get Colab secrets
        aws_access_key = None
        aws_secret_key = None
        aws_region = None
        aws_bucket = None
        
        # Method 1: Try google.colab.userdata (newer Colab)
        try:
            from google.colab import userdata
            aws_access_key = userdata.get('aws_access_key_id')
            aws_secret_key = userdata.get('aws_secret_access_key')
            aws_region = userdata.get('aws_region')
            aws_bucket = userdata.get('aws_s3_bucket')
            print("âœ… Loaded AWS credentials from google.colab.userdata")
        except ImportError:
            print("âš ï¸ google.colab.userdata not available")
        
        # Method 2: Try kaggle_secrets (Kaggle environment)
        if not aws_access_key:
            try:
                from kaggle_secrets import UserSecretsClient
                user_secrets = UserSecretsClient()
                aws_access_key = user_secrets.get_secret("aws_access_key_id")
                aws_secret_key = user_secrets.get_secret("aws_secret_access_key")
                aws_region = user_secrets.get_secret("aws_region")
                aws_bucket = user_secrets.get_secret("aws_s3_bucket")
                print("âœ… Loaded AWS credentials from kaggle_secrets")
            except ImportError:
                print("âš ï¸ kaggle_secrets not available")
        
        # Method 3: Try environment variables (already set)
        if not aws_access_key:
            aws_access_key = os.environ.get('AWS_ACCESS_KEY_ID')
            aws_secret_key = os.environ.get('AWS_SECRET_ACCESS_KEY')
            aws_region = os.environ.get('AWS_REGION')
            aws_bucket = os.environ.get('AWS_S3_BUCKET')
            if aws_access_key:
                print("âœ… AWS credentials found in environment variables")
        
        # Set environment variables if we found credentials
        if aws_access_key and aws_secret_key:
            os.environ['AWS_ACCESS_KEY_ID'] = aws_access_key
            os.environ['AWS_SECRET_ACCESS_KEY'] = aws_secret_key
            os.environ['AWS_REGION'] = aws_region or 'us-east-1'
            if aws_bucket:
                os.environ['AWS_S3_BUCKET'] = aws_bucket
            print("âœ… AWS credentials loaded successfully")
            return True
        else:
            print("âŒ No AWS credentials found in any source")
            return False
            
    except Exception as e:
        print(f"âŒ Failed to load AWS credentials from secrets: {e}")
        return False

def setup_aws_credentials_manual() -> bool:
    """
    Set up AWS credentials manually (for development/testing).
    
    Returns:
        bool: True if credentials were set up successfully
    """
    try:
        import os
        import json
        from pathlib import Path
        
        # Create .env/aws directory
        os.makedirs('.env/aws', exist_ok=True)
        
        # Check if credentials file already exists
        creds_path = Path('.env/aws/credentials.json')
        if creds_path.exists():
            print("âœ… AWS credentials file already exists")
            return True
        
        print("ğŸ“‹ Manual AWS credentials setup:")
        print("Please create .env/aws/credentials.json with the following structure:")
        print("""
{
    "aws_access_key_id": "YOUR_ACCESS_KEY_ID",
    "aws_secret_access_key": "YOUR_SECRET_ACCESS_KEY", 
    "region_name": "us-east-1",
    "s3_bucket": "YOUR_BUCKET_NAME"
}
        """)
        
        return False
        
    except Exception as e:
        print(f"âŒ Failed to set up AWS credentials: {e}")
        return False

def verify_aws_setup() -> Dict[str, Any]:
    """
    Verify AWS setup and credentials.
    
    Returns:
        Dict containing AWS setup verification results
    """
    print("ğŸ” Verifying AWS setup...")
    
    results = {
        'credentials_loaded': False,
        's3_accessible': False,
        'bucket_exists': False,
        'region': None,
        'bucket_name': None
    }
    
    try:
        import boto3
        from botocore.exceptions import NoCredentialsError, ClientError
        
        # Check if credentials are available
        sts_client = boto3.client('sts')
        identity = sts_client.get_caller_identity()
        
        results['credentials_loaded'] = True
        results['account_id'] = identity['Account']
        results['user_arn'] = identity['Arn']
        
        print(f"âœ… AWS credentials valid for account: {identity['Account']}")
        
        # Check S3 access
        s3_client = boto3.client('s3')
        s3_client.list_buckets()
        results['s3_accessible'] = True
        print("âœ… S3 access verified")
        
        # Check specific bucket
        bucket_name = os.environ.get('AWS_S3_BUCKET')
        if bucket_name:
            try:
                s3_client.head_bucket(Bucket=bucket_name)
                results['bucket_exists'] = True
                results['bucket_name'] = bucket_name
                print(f"âœ… S3 bucket '{bucket_name}' exists and accessible")
            except ClientError as e:
                print(f"âš ï¸ S3 bucket '{bucket_name}' not accessible: {e}")
        
        # Get region
        region = os.environ.get('AWS_REGION', 'us-east-1')
        results['region'] = region
        print(f"âœ… Using AWS region: {region}")
        
    except NoCredentialsError:
        print("âŒ AWS credentials not found")
    except Exception as e:
        print(f"âŒ AWS verification failed: {e}")
    
    return results

def quick_colab_setup(
    use_s3: bool = True,
    mount_drive: bool = True,
    run_tests: bool = True,
    force_reprocess: bool = False
) -> Dict[str, Any]:
    """
    Quick Colab setup that skips preprocessing if data already exists.
    
    Args:
        use_s3: Whether to use S3 for data operations
        mount_drive: Whether to mount Google Drive
        run_tests: Whether to run tests after setup
        force_reprocess: Whether to force reprocessing even if data exists
        
    Returns:
        Dict containing setup results
    """
    print("âš¡ Quick Colab Setup - Skipping preprocessing if data exists")
    print("="*60)
    
    return complete_colab_setup(
        data_path='/content/YaleGWI/train_samples',
        use_s3=use_s3,
        mount_drive=mount_drive,
        download_dataset=False,  # Skip dataset download
        dataset_source='manual',
        setup_aws=use_s3,
        run_tests=run_tests,
        force_reprocess=force_reprocess
    )

if __name__ == "__main__":
    # Example usage
    results = complete_colab_setup(use_s3=True)
    print("\nSetup completed with results:", results) 