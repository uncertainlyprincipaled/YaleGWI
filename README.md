# SpecProj-UNet for Seismic Waveform Inversion

## Table of Contents
1. [Overview](#1-overview)
2. [Project Status](#2-project-status)
3. [Quick Start (Local Development)](#3-quick-start-local-development)
4. [Multi-Platform Deployment](#4-multi-platform-deployment)
    - [Kaggle](#kaggle-environment)
    - [Google Colab](#google-colab-environment)
    - [Lambda Labs](#lambda-labs-environment)
    - [AWS EC2](#aws-ec2-environment)
    - [AWS SageMaker](#aws-sagemaker-environment)
5. [Additional Documentation](#5-additional-documentation)
    - [Usage Examples](#usage-examples)
    - [Model Tracking](#model-tracking)
    - [AWS Management Tools](#aws-management-tools)

## 1. Overview

A deep learning solution for seismic waveform inversion using a HGNet/ConvNeXt backbone with:
- EMA (Exponential Moving Average) for stable training
- AMP (Automatic Mixed Precision) for faster training
- Distributed Data Parallel (DDP) for multi-GPU training
- Test Time Augmentation (TTA) for improved inference
- **âœ… Phase 1 Complete**: Geometric-aware preprocessing and model management
- **âœ… Phase 1 Complete**: Family-specific data loading with geometric features
- **âœ… Phase 1 Complete**: Cross-validation with geometric metrics
- **ðŸš€ Phase 2 Ready**: EGNN, Heat-Kernel, and SE(3)-Transformer models
- **ðŸš€ Phase 3 Ready**: Ensemble framework with CRF integration
- **ðŸ’° Cost-Optimized**: Multi-platform training (Kaggle + Lambda Labs)

### Data IO Policy (IMPORTANT)
**All data loading, streaming, and batching must go through `DataManager` in `src/core/data_manager.py`.**
- Do NOT load data directly in any other file.
- This ensures memory efficiency and prevents RAM spikes in Kaggle.
- All source files must import and use DataManager for any data access.

### Project Structure
```
src/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ preprocess.py      # âœ… Geometric-aware preprocessing (FIXED)
â”‚   â”œâ”€â”€ registry.py        # âœ… Model registry with geometric metadata
â”‚   â”œâ”€â”€ checkpoint.py      # âœ… Checkpoint management
â”‚   â”œâ”€â”€ geometric_loader.py # âœ… Family-specific data loading
â”‚   â”œâ”€â”€ geometric_cv.py    # âœ… Cross-validation framework
â”‚   â”œâ”€â”€ data_manager.py    # âœ… Data IO management
â”‚   â”œâ”€â”€ egnn.py           # ðŸš€ E(n)-equivariant graph neural network
â”‚   â”œâ”€â”€ heat_kernel.py    # ðŸš€ Heat-kernel diffusion network
â”‚   â”œâ”€â”€ se3_transformer.py # ðŸš€ SE(3)-transformer for 3D equivariance
â”‚   â”œâ”€â”€ ensemble.py       # ðŸš€ Ensemble framework with CRF
â”‚   â”œâ”€â”€ crf.py           # ðŸš€ Conditional random field integration
â”‚   â”œâ”€â”€ model.py          # Model architecture
â”‚   â”œâ”€â”€ train.py          # Training pipeline (enhanced)
â”‚   â”œâ”€â”€ train_lambda.py   # ðŸš€ Lambda Labs training script
â”‚   â””â”€â”€ config.py         # Configuration
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ colab_setup.py    # âœ… Smart preprocessing setup
â”‚   â””â”€â”€ update_kaggle_notebook.py  # Notebook update script
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ run_tests.py      # âœ… Comprehensive testing
â”‚   â””â”€â”€ test_phase1_integration.py # âœ… Phase 1 integration tests
â”œâ”€â”€ kaggle_training.py    # ðŸš€ Kaggle-optimized training script
â””â”€â”€ requirements.txt      # Project dependencies
```

### Features
- **Physics-Guided Architecture**: Uses spectral projectors to inject wave-equation priors
- **Mixed Precision Training**: Supports both fp16 and bfloat16 for efficient training
- **Memory Optimization**: Implements efficient attention and memory management
- **Family-Aware Training**: Stratified sampling by geological families
- **Robust Training**: Includes gradient clipping, early stopping, and learning rate scheduling
- **Comprehensive Checkpointing**: Saves full training state for easy resumption
- **âœ… Geometric-Aware Processing**: Nyquist validation, geometric feature extraction
- **âœ… Model Registry**: Version control with geometric metadata
- **âœ… Cross-Validation**: Family-based stratification with geometric metrics
- **ðŸš€ Multi-Platform Training**: Kaggle (free) + Lambda Labs (paid) integration
- **ðŸš€ Smart Preprocessing**: Automatic skip if data exists, S3/Drive sync

---

## 2. Project Status

### âœ… **Phase 1: Core Infrastructure - COMPLETE**
- **Model Registry**: âœ… Working with geometric metadata
- **Checkpoint Manager**: âœ… Working with geometric-aware checkpointing
- **Family Data Loader**: âœ… Working with geometric feature extraction
- **Cross-Validation Framework**: âœ… Working with geometric metrics
- **Preprocessing Pipeline**: âœ… FIXED - Working with S3 data
- **Smart Preprocessing**: âœ… Automatic skip if data exists
- **All Tests**: âœ… PASSING

### ðŸš€ **Phase 2: Model Components - READY**
- **EGNN**: âœ… Implemented and tested
- **Heat-Kernel Diffusion**: âœ… Implemented and tested
- **SE(3)-Transformer**: âœ… Implemented and tested

### ðŸš€ **Phase 3: Ensemble Framework - READY**
- **Ensemble Base**: âœ… Implemented and tested
- **CRF Integration**: âœ… Implemented and tested
- **Bayesian Uncertainty**: âœ… Implemented and tested

### ðŸŽ¯ **Next Steps: Phase 4 Training**
1. **Kaggle Training**: SpecProj-UNet + Heat-Kernel (FREE)
2. **Lambda Labs Training**: EGNN + SE(3)-Transformer (~$200)
3. **Ensemble Training**: All models combined
4. **Final Submission**: Kaggle dataset with all models

### ðŸ“ˆ **Performance Improvements**
- **Aliasing Reduced**: From 10-13% to 5-7% (much better!)
- **All Components Working**: 100% test pass rate
- **S3 Integration**: Seamless data loading
- **Multi-Platform Ready**: Kaggle + Lambda Labs strategy

---

## 3. Quick Start (Local Development)

### Basic Usage

#### Preprocessing
```python
from src.core.preprocess import main as preprocess_main
# Run preprocessing with default settings
preprocess_main()  # This will create GPU-specific datasets
```

#### Training
```python
from src.core.train import train
# Start training with default settings
train(fp16=True)  # Enable mixed precision training
```

#### Inference
```python
from src.core.model import get_model
import torch
# Load model and weights
model = get_model()
model.load_state_dict(torch.load('outputs/best.pth'))
# Run inference
predictions = model(input_data)
```

---

## 4. Multi-Platform Deployment

### Overview of Training Strategy
Our training strategy leverages both free and paid resources for maximum efficiency:
- **Kaggle (FREE)**: Less computationally intensive models
- **Lambda Labs (~$200)**: More intensive models + ensemble training
- **Total Cost**: ~$200 instead of ~$400-1000

#### S3 Sync Strategy
- All models automatically export to S3
- Final ensemble downloads all models from S3
- Kaggle dataset created with all models for submission

#### 5-7 Day Timeline
1. **Day 1-2**: Setup and start Kaggle training
2. **Day 3-4**: Parallel training on all platforms
3. **Day 5-6**: Ensemble training on Lambda Labs
4. **Day 7**: Export and Kaggle submission

---

### Kaggle Environment
This project is optimized for the Kaggle environment.

#### Model & Cost Allocation
```python
# Models: SpecProj-UNet + Heat-Kernel
# Training Time: ~10-14 hours total
# Cost: $0
```

#### Quick Start: Full Pipeline
1. Create a new notebook in the [Waveform Inversion competition](https://www.kaggle.com/competitions/waveform-inversion)
2. **Important**: Add the required datasets to your notebook first:
   - Click on the 'Data' tab
   - Click 'Add Data'
   - Search for and add:
     1. 'Waveform Inversion' competition dataset
     2. 'openfwi-preprocessed-72x72' dataset (contains preprocessed data and pretrained models)
3. **Clone the repository**:
```python
!git clone -b dev https://github.com/uncertainlyprincipaled/YaleGWI.git
%cd YaleGWI
```
4. **Install dependencies using pip (more reliable in Kaggle)**:
```python
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
!pip install numpy pandas matplotlib tqdm pytest boto3 botocore awscli zarr dask scipy s3fs psutil timm einops polars watchdog omegaconf
!pip install kagglehub google-auth-oauthlib google-auth-httplib2 google-api-python-client monai pytorch-lightning==2.0.0 torchmetrics==0.11.4 segmentation-models-pytorch webdataset plotly packaging
```
5. Setting up AWS Credentials in Kaggle
   1. Go to your Kaggle account settings
   2. Click on "Add-ons" tab
   3. Click on "Secrets" in the left sidebar
   4. Add the following secrets:
      - `aws_access_key_id`: Your AWS access key ID
      - `aws_secret_access_key`: Your AWS secret access key
      - `aws_region`: Your AWS region (optional, defaults to 'us-east-1')
      - `aws_s3_bucket`: Your S3 bucket name for storing preprocessed data

   5. Load AWS credentials from kaggle secrets
   ```python
   from kaggle_secrets import UserSecretsClient
   user_secrets = UserSecretsClient()

   import os
   os.environ['AWS_ACCESS_KEY_ID'] = user_secrets.get_secret("aws_access_key_id")
   os.environ['AWS_SECRET_ACCESS_KEY'] = user_secrets.get_secret("aws_secret_access_key")
   os.environ['AWS_REGION'] = user_secrets.get_secret("aws_region")
   os.environ['AWS_S3_BUCKET'] = user_secrets.get_secret("aws_s3_bucket")

   print("âœ… AWS credentials loaded")'''

6. **Run preprocessing with S3 offloading**:
```python
!pip install zarr
import zarr
from src.core.preprocess import main as preprocess_main
preprocess_main()  # Will use S3 bucket from Kaggle secrets
```
#### Verify Preprocessed Data from S3
After preprocessing, the script uploads the data to S3. You can verify the data integrity on S3 directly using our verification script. This requires your AWS credentials to be set as Kaggle secrets (see below).

```bash
# Ensure you have set your AWS credentials as Kaggle secrets
# Then, run the verification script:
python -m tests.test_verify_s3_preprocess
```
7. **Start training**:
```python
# Train SpecProj-UNet
!python kaggle_training.py --model specproj_unet --epochs 30 --keep-alive

# Train Heat-Kernel Model  
!python kaggle_training.py --model heat_kernel --epochs 30 --keep-alive
```

#### Notebook Organization
The project uses a notebook update script (`update_kaggle_notebook.py`) that automatically organizes code into cells in the correct order:
1. Imports and setup
2. Preprocessing
3. Model registry and checkpoint management
4. Data loading and geometric features
5. Training configuration
6. Training loop
7. Inference and submission

---

### Google Colab Environment

> **ðŸ’¡ Pro Tip**: Phase 1 is now COMPLETE! All components are working and tested. Ready for Phase 2 & 3 training.

#### Quick Start: Smart Preprocessing Workflow

##### Option A: Quick Setup (Recommended for Repeated Runs)
```python
# One-command setup that skips preprocessing if data already exists
!git clone -b dev https://github.com/uncertainlyprincipaled/YaleGWI.git
%cd YaleGWI

# Quick setup - skips preprocessing if data exists locally or in Google Drive
from src.utils.colab_setup import quick_colab_setup
results = quick_colab_setup(
    use_s3=True,           # Use S3 for data operations
    mount_drive=True,      # Mount Google Drive for persistent storage
    run_tests=True,        # Run validation tests
    force_reprocess=False  # Skip preprocessing if data exists
)

# Force reprocessing after config changes
results = quick_colab_setup(use_s3=True, force_reprocess=True)

# Check data status manually
from src.utils.colab_setup import check_preprocessed_data_exists
status = check_preprocessed_data_exists('/content/YaleGWI/preprocessed', save_to_drive=True, use_s3=True)
```

##### Option B: Full Automated Setup (First Time)
```python
# Complete setup with preprocessing
from src.utils.colab_setup import complete_colab_setup
results = complete_colab_setup(
    data_path='/content/YaleGWI/train_samples',
    use_s3=True,           # Use S3 for data operations
    mount_drive=True,      # Mount Google Drive for persistent storage
    download_dataset=False, # Set to True if you need to download the dataset
    setup_aws=True,        # Load AWS credentials from Colab secrets
    run_tests=True,        # Run validation tests
    force_reprocess=False  # Skip preprocessing if data exists
)
```

**Smart Preprocessing Features:**
- âœ… **Automatic Detection**: Checks local, Google Drive, and S3 for existing data
- âœ… **Intelligent Skip**: Skips preprocessing if data already exists
- âœ… **Efficient Copy**: Copies from Google Drive to local if needed
- âœ… **Force Option**: Override skip behavior when needed
- âœ… **Data Validation**: Verifies data quality before skipping
- âœ… **Robust Installation**: Handles missing requirements.txt gracefully
- âœ… **Flexible AWS Setup**: Works with different Colab secret methods

**Phase 1 Status: âœ… COMPLETE**
- âœ… All Phase 1 tests passing
- âœ… Preprocessing working with reduced aliasing (5-7% vs 10-13%)
- âœ… All components functional and tested
- âœ… Ready for Phase 2 & 3 training

**AWS Credentials Setup (Optional for S3):**
The setup automatically tries multiple methods to find AWS credentials:

1. **Google Colab Secrets** (Recommended):
   - Go to the left sidebar in Colab
   - Click on the "Secrets" icon (ðŸ”‘)
   - Add these secrets:
     - `aws_access_key_id`: Your AWS access key ID
     - `aws_secret_access_key`: Your AWS secret access key
     - `aws_region`: Your AWS region (e.g., us-east-1)
     - `aws_s3_bucket`: Your S3 bucket name

2. **Environment Variables**: If already set in the environment
3. **Manual Setup**: Creates `.env/aws/credentials.json` file

**If AWS credentials are not available:**
- The setup will offer to switch to local processing mode
- You can still use Google Drive for persistent storage
- All preprocessing and training will work locally

#### Troubleshooting Setup Issues

##### Package Installation Errors
If you encounter package installation errors:
```python
# Test the setup components
!python test_colab_setup.py

# Or install packages manually
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
!pip install numpy pandas matplotlib tqdm pytest boto3 botocore awscli zarr dask scipy s3fs psutil timm einops polars watchdog omegaconf
!pip install kagglehub google-auth-oauthlib google-auth-httplib2 google-api-python-client monai pytorch-lightning==2.0.0 torchmetrics==0.11.4 segmentation-models-pytorch webdataset plotly packaging
```

##### AWS Credentials Issues
If AWS setup fails:
```python
# Check what credentials are available
import os
print("AWS_ACCESS_KEY_ID:", "SET" if os.environ.get('AWS_ACCESS_KEY_ID') else "NOT SET")
print("AWS_SECRET_ACCESS_KEY:", "SET" if os.environ.get('AWS_SECRET_ACCESS_KEY') else "NOT SET")
print("AWS_REGION:", os.environ.get('AWS_REGION', 'NOT SET'))
print("AWS_S3_BUCKET:", os.environ.get('AWS_S3_BUCKET', 'NOT SET'))

# Continue with local processing
results = quick_colab_setup(use_s3=False, mount_drive=True)
```

##### Zarr Compression Issues
If you encounter `module 'zarr' has no attribute 'Blosc'` errors:
```python
# The code has been updated to handle this automatically
# But if you still have issues, try these steps:

# 1. Test the zarr fix
!python test_zarr_fix.py

# 2. Reinstall zarr if needed
!pip install --upgrade zarr

# 3. Restart runtime and try again
# Runtime -> Restart runtime

# 4. Run setup again
from src.utils.colab_setup import quick_colab_setup
results = quick_colab_setup(use_s3=True, mount_drive=True)
```

If you encounter `got multiple values for keyword argument 'chunks'` errors:
```python
# This is a dask-zarr compatibility issue that has been fixed
# The code now properly handles chunking without conflicts

# 1. Test the fix
!python test_zarr_fix.py

# 2. Run setup again (should work now)
from src.utils.colab_setup import quick_colab_setup
results = quick_colab_setup(use_s3=True, mount_drive=True)
```

The preprocessing pipeline now automatically handles zarr compression issues by:
- Trying default compression first
- Falling back to no compression if needed
- Using computed arrays as final fallback
- Providing clear error messages and guidance

---

### Lambda Labs Environment

This section guides you through setting up and running your training on a Lambda Labs cloud instance.

#### Model & Cost Allocation
```bash
# Models: EGNN + SE(3)-Transformer + Ensemble
# Training Time: ~2-3 days total
# Cost: ~$200
```

#### 1. SSH Key Setup
Lambda Labs requires a modern SSH key type. The `ssh-rsa` standard is often deprecated. We will use `ed25519`.

**A. Generate the Key**
Run the following command on your local machine. Use a strong passphrase when prompted.
```bash
ssh-keygen -t ed25519 -f ~/.ssh/lambda_labs_ed25519 -C "your_email@example.com"
```

**B. Add Public Key to Lambda Labs**
Copy the entire contents of your **public** key and add it to your Lambda Labs account.
```bash
cat ~/.ssh/lambda_labs_ed25519.pub
```
1.  Go to the [SSH Keys page](https://cloud.lambdalabs.com/ssh-keys) on your Lambda Labs dashboard.
2.  Click "Add SSH Key".
3.  Paste the copied public key into the text field.
4.  Give it a memorable name (e.g., "MacBook Main").

#### 2. Launching an Instance
**A. Choose an Instance Type**
For the intensive models (EGNN, SE(3)-Transformer, Ensemble), a powerful GPU is recommended.
- **Best Value**: **RTX 6000 Ada** ($0.69/hr) offers 48GB of VRAM, which is excellent for large models.
- **Budget Option**: **A10** ($0.59/hr) has 24GB of VRAM, which may require smaller batch sizes.

**B. Launch**
1.  Go to the [Instances page](https://cloud.lambdalabs.com/instances) and click "Launch Instance".
2.  Select your desired GPU type and region.
3.  Ensure your new `ed25519` SSH key is selected.
4.  For filesystem, select **"Don't attach a filesystem"** for initial testing and ephemeral jobs. For persistent storage across sessions, you can create and attach a filesystem.
5.  Click "Launch".

#### 3. Connect and Run Setup
Once the instance is running, copy its IP address.

**A. SSH into the Instance**
The default username is `ubuntu`. Use the `ed25519` key you created.
```bash
ssh -i ~/.ssh/lambda_train_key ubuntu@YOUR_INSTANCE_IP
```
The first time you connect, you will be asked to verify the host's authenticity. Type `yes`. Then, enter your SSH key's passphrase.

**B. Run the Setup Script**
Once you are logged in, clone the repository and run the setup script. This will automatically install Mamba and set up the environment using `environment.yml`.
```bash
# 1. Clone the repository
git clone -b dev https://github.com/uncertainlyprincipaled/YaleGWI.git

# 2. Navigate into the directory
cd YaleGWI

# 3. Make the setup script executable and run it
chmod +x scripts/setup_lambda.sh && ./scripts/setup_lambda.sh
```
The script will prompt you to configure AWS credentials if you wish to connect to S3.

#### 4. Verify Preprocessed Data from S3
After setting up the instance, you can verify your preprocessed data on S3 using the dedicated test script.
```bash
# 1. Ensure AWS credentials are set in your environment
export AWS_ACCESS_KEY_ID="YOUR_KEY"
export AWS_SECRET_ACCESS_KEY="YOUR_SECRET"
export AWS_REGION="us-east-1"

# 2. Run the verification script
python -m tests.test_verify_s3_preprocess
```

#### 5. Start Training
You can now run your training jobs. The `train_lambda.sh` script created by the setup will automatically activate the correct Mamba environment.
```bash
# Train individual models
./train_lambda.sh --model egnn --epochs 30
# ... existing code ...
4. **Set Up Python Environment with Mamba**:
   ```bash
   # Mamba is recommended for fast installation
   wget "https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-$(uname)-$(uname -m).sh"
   bash Mambaforge-$(uname)-$(uname -m).sh -b -p "${HOME}/mambaforge"
   source "${HOME}/mambaforge/bin/activate"
   mamba init
   
   # After restarting shell, create the environment
   mamba env create -f environment.yml
   conda activate YaleGWI
   ```

5. **Configure AWS Credentials**
   ```bash
   # ... existing code ...
   ```

---

### AWS EC2 Environment
#### Quick Start: Full Pipeline
1. **Preprocessing**
   ```python
   from src.core.preprocess import load_data
   load_data('/mnt/waveform-inversion/train_samples', '/mnt/output/preprocessed', use_s3=True)
   ```
2. **Training**
   ```python
   from src.core.train_ec2 import main as train_ec2_main
   train_ec2_main()
   ```
3. **Inference**
   ```python
   from src.core.model import get_model
   import torch
   model = get_model()
   model.load_state_dict(torch.load('outputs/best.pth'))
   predictions = model(input_data)
   ```

#### Detailed Setup: Multi-GPU Training on AWS EC2 (g4dn.12xlarge)
1. **Launch a New EC2 Instance**
   - Use a GPU-enabled instance: **g4dn.12xlarge** (4 Ã— NVIDIA T4, 16GB each)
   - Use Ubuntu 22.04 or 20.04 AMI
   - Attach or create a 200GB+ EBS volume for data

2. **SSH into the Instance**
   ```bash
   ssh -i .env/aws/your-key.pem ubuntu@instance-ip
   ```

3. **Clone the Repository**
   ```bash
   git clone https://github.com/uncertainlyprincipaled/YaleGWI.git
   cd YaleGWI
   ```

4. **Set Up Python Environment**
   ```bash
   sudo apt-get update && sudo apt-get install -y python3-venv
   python3 -m venv venv
   source venv/bin/activate
   pip install --upgrade pip
   pip install -r requirements.txt
   ```

5. **Configure AWS Credentials**
   ```bash
   source .env/aws/credentials
   ```

6. **Download/Sync Data from S3**
   ```bash
   sudo mkdir -p /mnt/waveform-inversion
   sudo mkdir -p /mnt/output
   sudo chown $USER:$USER /mnt/waveform-inversion
   sudo chown $USER:$USER /mnt/output
   python -m src.core.setup aws
   ```

7. **Multi-GPU Training (Distributed Data Parallel)**
   - The codebase now supports multi-GPU training using PyTorch DDP.
   - Use the provided `train_ec2.py` script for EC2 multi-GPU training:
   ```bash
   export GWI_ENV=aws
   python -m torch.distributed.launch --nproc_per_node=4 src/core/train_ec2.py --epochs 120 --batch 4 --amp --experiment-tag "run-$(date +%F)"
   ```
   - This will launch 4 processes (one per GPU) and train in parallel.

---

### AWS SageMaker Environment

#### Quick Start: Full Pipeline
1. **Preprocessing**
   ```python
   from src.core.preprocess import load_data
   load_data('/home/sagemaker-user/YaleGWI/train_samples', '/home/sagemaker-user/YaleGWI/preprocessed', use_s3=True)
   ```
2. **Training**
   ```python
   from src.core.train import train
   train(fp16=True)
   ```
3. **Inference**
   ```python
   from src.core.model import get_model
   import torch
   model = get_model()
   model.load_state_dict(torch.load('outputs/best.pth'))
   predictions = model(input_data)
   ```

#### Detailed Setup
1. **Create SageMaker Notebook Instance**:
   - Go to AWS SageMaker Console
   - Create new notebook instance
   - Choose `ml.c5.2xlarge` (CPU-only instance)
   - Set volume size to 100GB
   - Create new IAM role with S3 access
   - Launch instance

2. **Clone Repository**:
   ```bash
   # Open terminal in JupyterLab and run:
   git clone -b dev https://github.com/uncertainlyprincipaled/YaleGWI.git
   cd YaleGWI
   ```

3. **Import All Core Libraries in Your Notebook**:
   ```python
   import os
   import sys
   import json
   import boto3
   from botocore.exceptions import ClientError

   # 1. Set environment variable for config
   os.environ['GWI_ENV'] = 'sagemaker'

   # 2. Set up project path
   os.chdir('/home/sagemaker-user/YaleGWI')  # Adjust as needed
   sys.path.append(os.path.abspath('./src'))

   # 2. Retrieve secret from AWS Secrets Manager
   def get_secret():
       secret_name = "sagemaker-access"  # Use your actual secret name
       region_name = "us-east-1"              # Use your region

       session = boto3.session.Session()
       client = session.client(
           service_name='secretsmanager',
           region_name=region_name
       )

       try:
           get_secret_value_response = client.get_secret_value(
               SecretId=secret_name
           )
       except ClientError as e:
           raise e

       return json.loads(get_secret_value_response['SecretString'])

   secret = get_secret()

   # 3. Set environment variables for DataManager and boto3
   os.environ['AWS_ACCESS_KEY_ID'] = secret['aws_access_key_id']
   os.environ['AWS_SECRET_ACCESS_KEY'] = secret['aws_secret_access_key']
   os.environ['AWS_REGION'] = secret['region_name']
   os.environ['AWS_S3_BUCKET'] = secret['s3_bucket']

   # 4. Import all core libraries
   from src.core.imports import *
   # Now you have np, torch, CFG, DataManager, etc. available
   ```

---

## 5. Additional Documentation

### Usage Examples
- For large datasets, use the provided preprocessing script to create GPU-specific datasets
- The preprocessing includes Nyquist validation and geometric feature extraction
- Processed data is saved in zarr format for efficient loading
- Use the geometric-aware data loader for family-specific training

### Model Tracking
> **Note:** MLflow tracking and logging is abstracted away into a utility class. In most workflows (including EC2 and cloud), users do not need to write MLflow code directly. All model versioning, metric logging, and artifact management is handled automatically by the pipeline.
- Install MLflow: `pip install mlflow`
- Set up tracking server (optional)
- Configure experiment tracking

### AWS Management Tools

#### AWS Service Quota Checking
Before running resource-intensive operations, check your AWS service quotas:
```bash
# Make script executable (first time only)
chmod +x scripts/check_aws_quotas.sh

# Check quotas for default region (us-east-1)
./scripts/check_aws_quotas.sh
```
**Important quotas to monitor**:
- EC2 instance limits (especially G and P instances for GPU training)
- EBS storage limits
- S3 bucket and object limits
- SageMaker training job limits

#### EBS Volume Cleanup
To manage costs and clean up unused EBS volumes:
```bash
# Make script executable (first time only)
chmod +x scripts/cleanup_ebs_volumes.sh

# Dry run: see what would be deleted
./scripts/cleanup_ebs_volumes.sh -a

# Delete all unattached volumes (with confirmation)
./scripts/cleanup_ebs_volumes.sh -a -x
```
**Safety Features**:
- **Dry-run mode by default** - shows what would be deleted without actually deleting
- **Confirmation prompts** - asks for confirmation before deletion
- **Safety checks** - won't delete volumes that are attached, have snapshots, or are in use.
- **Cost estimation** - shows potential cost savings
- **Detailed reporting** - provides comprehensive results